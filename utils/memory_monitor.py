"""
ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°
K-Fold í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì •ë¦¬
"""

import torch
import gc
import psutil
import os
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)


class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì •ë¦¬"""

    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.initial_memory = self.get_memory_info()

    def get_memory_info(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        info = {}

        # ì‹œìŠ¤í…œ RAM
        memory_info = self.process.memory_info()
        info['ram_used_gb'] = memory_info.rss / 1024**3
        info['ram_percent'] = self.process.memory_percent()

        # GPU ë©”ëª¨ë¦¬
        if torch.cuda.is_available():
            info['gpu_allocated_gb'] = torch.cuda.memory_allocated() / 1024**3
            info['gpu_cached_gb'] = torch.cuda.memory_reserved() / 1024**3
            info['gpu_total_gb'] = torch.cuda.get_device_properties(0).total_memory / 1024**3
            info['gpu_free_gb'] = info['gpu_total_gb'] - info['gpu_allocated_gb']
        else:
            info['gpu_allocated_gb'] = 0
            info['gpu_cached_gb'] = 0
            info['gpu_total_gb'] = 0
            info['gpu_free_gb'] = 0

        return info

    def print_memory_status(self, prefix: str = ""):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
        info = self.get_memory_info()

        print(f"\nğŸ“Š {prefix} ë©”ëª¨ë¦¬ ìƒíƒœ:")
        print(f"   ğŸ’¾ RAM: {info['ram_used_gb']:.2f}GB ({info['ram_percent']:.1f}%)")

        if torch.cuda.is_available():
            print(f"   ğŸ® GPU: {info['gpu_allocated_gb']:.2f}GB / {info['gpu_total_gb']:.2f}GB")
            print(f"   ğŸ“¦ GPU ìºì‹œ: {info['gpu_cached_gb']:.2f}GB")
            print(f"   ğŸ†“ GPU ì—¬ìœ : {info['gpu_free_gb']:.2f}GB")

            # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ 
            if info['gpu_free_gb'] < 1.0:
                print(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ ! (ì—¬ìœ : {info['gpu_free_gb']:.2f}GB)")

    def cleanup_memory(self, aggressive: bool = False):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")

        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        logger.info(f"   Python GC: {collected}ê°œ ê°ì²´ ì •ë¦¬")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            if aggressive:
                # ë” ì ê·¹ì ì¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.ipc_collect()

            logger.info("   GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

        # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
        self.print_memory_status("ì •ë¦¬ í›„")

    def check_memory_safety(self, min_gpu_gb: float = 2.0, min_ram_percent: float = 80.0) -> bool:
        """ë©”ëª¨ë¦¬ ì•ˆì „ì„± í™•ì¸"""
        info = self.get_memory_info()

        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if torch.cuda.is_available() and info['gpu_free_gb'] < min_gpu_gb:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {info['gpu_free_gb']:.2f}GB < {min_gpu_gb}GB")
            return False

        # RAM ì‚¬ìš©ë¥  í™•ì¸
        if info['ram_percent'] > min_ram_percent:
            logger.warning(f"RAM ì‚¬ìš©ë¥  ë†’ìŒ: {info['ram_percent']:.1f}% > {min_ram_percent}%")
            return False

        return True

    def wait_for_memory_recovery(self, max_wait_seconds: int = 30):
        """ë©”ëª¨ë¦¬ íšŒë³µ ëŒ€ê¸°"""
        import time

        logger.info("â³ ë©”ëª¨ë¦¬ íšŒë³µ ëŒ€ê¸° ì¤‘...")

        for i in range(max_wait_seconds):
            if self.check_memory_safety():
                logger.info(f"âœ… ë©”ëª¨ë¦¬ íšŒë³µ ì™„ë£Œ ({i+1}ì´ˆ ëŒ€ê¸°)")
                return True

            time.sleep(1)

            # 5ì´ˆë§ˆë‹¤ ì •ë¦¬ ì‹œë„
            if (i + 1) % 5 == 0:
                self.cleanup_memory(aggressive=True)

        logger.warning(f"âš ï¸ {max_wait_seconds}ì´ˆ ëŒ€ê¸° í›„ì—ë„ ë©”ëª¨ë¦¬ ë¶€ì¡±")
        return False


def safe_fold_training_wrapper(fold_training_func):
    """K-Fold í•™ìŠµì„ ë©”ëª¨ë¦¬ ì•ˆì „í•˜ê²Œ ë˜í•‘í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        monitor = MemoryMonitor()
        fold_num = args[1] if len(args) > 1 else kwargs.get('fold_num', 'unknown')

        try:
            # í•™ìŠµ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            monitor.print_memory_status(f"Fold {fold_num} í•™ìŠµ ì „")

            # ë©”ëª¨ë¦¬ ì•ˆì „ì„± í™•ì¸
            if not monitor.check_memory_safety():
                logger.warning(f"Fold {fold_num} ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ë¶€ì¡±, ì •ë¦¬ ì‹œë„...")
                monitor.cleanup_memory(aggressive=True)
                monitor.wait_for_memory_recovery()

            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
            result = fold_training_func(*args, **kwargs)

            # í•™ìŠµ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            monitor.print_memory_status(f"Fold {fold_num} í•™ìŠµ í›„")
            monitor.cleanup_memory(aggressive=True)

            return result

        except Exception as e:
            logger.error(f"Fold {fold_num} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            monitor.cleanup_memory(aggressive=True)
            raise

    return wrapper


# ì „ì—­ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°
global_monitor = MemoryMonitor()


def print_memory_status(prefix: str = ""):
    """ì „ì—­ ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
    global_monitor.print_memory_status(prefix)


def cleanup_memory(aggressive: bool = False):
    """ì „ì—­ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    global_monitor.cleanup_memory(aggressive)


def check_memory_safety(min_gpu_gb: float = 2.0) -> bool:
    """ì „ì—­ ë©”ëª¨ë¦¬ ì•ˆì „ì„± í™•ì¸"""
    return global_monitor.check_memory_safety(min_gpu_gb)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    monitor = MemoryMonitor()
    monitor.print_memory_status("í…ŒìŠ¤íŠ¸")
    monitor.cleanup_memory()
    print("âœ… ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
