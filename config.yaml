general:
  data_path: ./data/augmented_google_ko-en-ko_10pct_20250803_182157
  model_name: digit82/kobart-summarization
  output_dir: ./model_output/
inference:
  batch_size: 32
  ckt_path: ./model_output/
  early_stopping: true
  generate_max_length: 100
  no_repeat_ngram_size: 2
  num_beams: 4
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  result_path: ./prediction/
kfold:
  enabled: false
  ensemble_method: voting
  n_splits: 5
  random_state: 42
  save_individual_models: true
  stratified: false
  use_ensemble_inference: true
tokenizer:
  bos_token: <s>
  decoder_max_len: 100
  encoder_max_len: 512
  eos_token: </s>
  special_tokens:
  - 'A:'
  - 'B:'
  - '#PhoneNumber#'
  - '#Address#'
  - '#DateOfBirth#'
  - '#PassportNumber#'
  - '#SSN#'
  - '#CardNumber#'
  - '#CarNumber#'
  - '#Email#'
training:
  do_eval: true
  do_train: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  evaluation_strategy: epoch
  fp16: true
  generation_max_length: 100
  gradient_accumulation_steps: 2
  greater_is_better: true
  learning_rate: 1.0e-05
  load_best_model_at_end: true
  logging_dir: ./logs
  logging_strategy: epoch
  lr_scheduler_type: cosine
  metric_for_best_model: eval_final_score
  num_train_epochs: 10
  optim: adamw_torch
  overwrite_output_dir: true
  per_device_eval_batch_size: 16
  per_device_train_batch_size: 16
  predict_with_generate: true
  report_to: none
  save_strategy: epoch
  save_total_limit: 3
  seed: 42
  warmup_ratio: 0.1
  weight_decay: 0.01
wandb:
  entity: your_entity
  name: baseline_run
  project: dialogue_summarization
