"""
ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
"""

import pandas as pd
import os
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer

class Preprocess:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, bos_token: str, eos_token: str):
        self.bos_token = bos_token
        self.eos_token = eos_token

    @staticmethod
    def make_set_as_df(file_path, is_train=True):
        """CSV íŒŒì¼ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜"""
        df = pd.read_csv(file_path)
        if is_train:
            return df[['fname', 'dialogue', 'summary']]
        else:
            return df[['fname', 'dialogue']]

    def make_input(self, dataset, is_test=False):
        """ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë°ì´í„° ë³€í™˜"""
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()

class DatasetForTrain(Dataset):
    """í•™ìŠµìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    def __init__(self, encoder_input, decoder_input, labels, length):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.length = length

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids')
        item2.pop('attention_mask')
        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]
        return item

    def __len__(self):
        return self.length

class DatasetForVal(Dataset):
    """ê²€ì¦ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    def __init__(self, encoder_input, decoder_input, labels, length):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.length = length

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids')
        item2.pop('attention_mask')
        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]
        return item

    def __len__(self):
        return self.length

class DatasetForInference(Dataset):
    """ì¶”ë¡ ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    def __init__(self, encoder_input, test_id, length):
        self.encoder_input = encoder_input
        self.test_id = test_id
        self.length = length

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item['ID'] = self.test_id[idx]
        return item

    def __len__(self):
        return self.length

class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    def __init__(self, config):
        self.config = config
        self.preprocessor = Preprocess(
            config['tokenizer']['bos_token'],
            config['tokenizer']['eos_token']
        )

    def prepare_train_dataset(self, tokenizer):
        """í•™ìŠµìš© ë°ì´í„°ì…‹ ì¤€ë¹„"""
        data_path = self.config['general']['data_path']

        print(f"ğŸ“ ë°ì´í„° ê²½ë¡œ: {data_path}")

        # ë°ì´í„° ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(data_path):
            print(f"âŒ ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            # ëŒ€ì²´ ê²½ë¡œë“¤ ì‹œë„
            alternative_paths = ["data/", "../data/", "./data/"]
            for alt_path in alternative_paths:
                if os.path.exists(alt_path):
                    data_path = alt_path
                    print(f"âœ… ëŒ€ì²´ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {data_path}")
                    break
            else:
                raise FileNotFoundError(f"ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”: {[data_path] + alternative_paths}")

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        train_file = os.path.join(data_path, 'train.csv')
        val_file = os.path.join(data_path, 'dev.csv')

        print(f"ğŸ” ì°¾ëŠ” íŒŒì¼: {train_file}")

        # ë°ì´í„° ë¡œë“œ
        train_data = self.preprocessor.make_set_as_df(train_file)
        val_data = self.preprocessor.make_set_as_df(val_file)

        print(f"í•™ìŠµ ë°ì´í„°: {len(train_data)} ìƒ˜í”Œ")
        print(f"ê²€ì¦ ë°ì´í„°: {len(val_data)} ìƒ˜í”Œ")

        # ìƒ˜í”Œ ì¶œë ¥
        print('-' * 100)
        print(f'í•™ìŠµ ëŒ€í™” ìƒ˜í”Œ:\n{train_data["dialogue"][0][:200]}...')
        print(f'í•™ìŠµ ìš”ì•½ ìƒ˜í”Œ:\n{train_data["summary"][0]}')

        # ì…ë ¥ ë°ì´í„° ìƒì„±
        encoder_input_train, decoder_input_train, decoder_output_train = self.preprocessor.make_input(train_data)
        encoder_input_val, decoder_input_val, decoder_output_val = self.preprocessor.make_input(val_data)

        # í† í¬ë‚˜ì´ì§•
        print("í† í¬ë‚˜ì´ì§• ì¤‘...")

        # í•™ìŠµ ë°ì´í„° í† í¬ë‚˜ì´ì§•
        tokenized_encoder_inputs = tokenizer(
            encoder_input_train,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['encoder_max_len'],
            return_token_type_ids=False
        )

        tokenized_decoder_inputs = tokenizer(
            decoder_input_train,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            return_token_type_ids=False
        )

        tokenized_decoder_outputs = tokenizer(
            decoder_output_train,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            return_token_type_ids=False
        )

        # ê²€ì¦ ë°ì´í„° í† í¬ë‚˜ì´ì§•
        val_tokenized_encoder_inputs = tokenizer(
            encoder_input_val,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['encoder_max_len'],
            return_token_type_ids=False
        )

        val_tokenized_decoder_inputs = tokenizer(
            decoder_input_val,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            return_token_type_ids=False
        )

        val_tokenized_decoder_outputs = tokenizer(
            decoder_output_val,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            return_token_type_ids=False
        )

        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = DatasetForTrain(
            tokenized_encoder_inputs,
            tokenized_decoder_inputs,
            tokenized_decoder_outputs,
            len(encoder_input_train)
        )

        val_dataset = DatasetForVal(
            val_tokenized_encoder_inputs,
            val_tokenized_decoder_inputs,
            val_tokenized_decoder_outputs,
            len(encoder_input_val)
        )

        print("ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
        return train_dataset, val_dataset

    def prepare_test_dataset(self, tokenizer, use_original=False):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ì¤€ë¹„"""
        data_path = self.config['general']['data_path']

        if use_original:
            # ì¸í¼ëŸ°ìŠ¤ëŠ” í•­ìƒ ì›ë³¸ test.csv ì‚¬ìš©
            test_file = os.path.join('data', 'test.csv')  # ì›ë³¸ ë°ì´í„° ê²½ë¡œ ê³ ì •
            print(f"ğŸ” ì¸í¼ëŸ°ìŠ¤ìš© ì›ë³¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©: {test_file}")
        else:
            # í•™ìŠµìš©ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
            test_file = os.path.join(data_path, 'test_advanced.csv')

            if not os.path.exists(test_file):
                test_file = os.path.join(data_path, 'test.csv')
                print(f"ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì¼ì´ ì—†ì–´ ì›ë³¸ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {test_file}")

        if not os.path.exists(test_file):
            raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}")

        test_data = self.preprocessor.make_set_as_df(test_file, is_train=False)
        test_id = test_data['fname']

        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)} ìƒ˜í”Œ")
        print('-' * 100)
        print(f'í…ŒìŠ¤íŠ¸ ëŒ€í™” ìƒ˜í”Œ:\n{test_data["dialogue"][0][:200]}...')

        encoder_input_test, decoder_input_test = self.preprocessor.make_input(test_data, is_test=True)

        # í† í¬ë‚˜ì´ì§•
        test_tokenized_encoder_inputs = tokenizer(
            encoder_input_test,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True,
            truncation=True,
            max_length=self.config['tokenizer']['encoder_max_len'],
            return_token_type_ids=False
        )

        test_dataset = DatasetForInference(
            test_tokenized_encoder_inputs,
            test_id,
            len(encoder_input_test)
        )

        return test_data, test_dataset

    def prepare_fold_dataset(self, tokenizer, train_path: str, val_path: str):
        """K-Foldìš© ë°ì´í„°ì…‹ ì¤€ë¹„"""
        import json

        print(f"ğŸ“ K-Fold ë°ì´í„° ë¡œë“œ: train={train_path}, val={val_path}")

        # JSON ë°ì´í„° ë¡œë“œ
        with open(train_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)

        with open(val_path, 'r', encoding='utf-8') as f:
            val_data = json.load(f)

        print(f"í•™ìŠµ ë°ì´í„°: {len(train_data)} ìƒ˜í”Œ")
        print(f"ê²€ì¦ ë°ì´í„°: {len(val_data)} ìƒ˜í”Œ")

        # JSON ë°ì´í„°ë¥¼ DataFrame í˜•íƒœë¡œ ë³€í™˜
        import pandas as pd

        train_df = pd.DataFrame(train_data)
        val_df = pd.DataFrame(val_data)

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë‹¤ì–‘í•œ JSON êµ¬ì¡° ì§€ì›)
        def normalize_dataframe(df, df_name):
            """DataFrameì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
            if 'dialogue' in df.columns and 'summary' in df.columns:
                # í‘œì¤€ í˜•ì‹
                return df[['dialogue', 'summary']].copy()
            elif 'input' in df.columns and 'output' in df.columns:
                # input/output í˜•ì‹
                normalized_df = df.rename(columns={'input': 'dialogue', 'output': 'summary'})
                return normalized_df[['dialogue', 'summary']].copy()
            else:
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹
                available_cols = df.columns.tolist()
                raise ValueError(f"{df_name} ë°ì´í„°ì˜ í˜•ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                               f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_cols}. "
                               f"í•„ìš”í•œ ì»¬ëŸ¼: ['dialogue', 'summary'] ë˜ëŠ” ['input', 'output']")

        train_df = normalize_dataframe(train_df, "í•™ìŠµ")
        val_df = normalize_dataframe(val_df, "ê²€ì¦")

        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë¬¸ìì—´ ë³€í™˜
        train_df['dialogue'] = train_df['dialogue'].astype(str)
        train_df['summary'] = train_df['summary'].astype(str)
        val_df['dialogue'] = val_df['dialogue'].astype(str)
        val_df['summary'] = val_df['summary'].astype(str)

        # ìƒ˜í”Œ ì¶œë ¥
        print('-' * 100)
        print(f'K-Fold í•™ìŠµ ëŒ€í™” ìƒ˜í”Œ:\n{train_df["dialogue"].iloc[0][:200]}...')
        print(f'K-Fold í•™ìŠµ ìš”ì•½ ìƒ˜í”Œ:\n{train_df["summary"].iloc[0]}')

        # ì…ë ¥ ë°ì´í„° ìƒì„±
        encoder_input_train, decoder_input_train, decoder_output_train = self.preprocessor.make_input(train_df)
        encoder_input_val, decoder_input_val, decoder_output_val = self.preprocessor.make_input(val_df)

        # í† í¬ë‚˜ì´ì§•
        print("K-Fold ë°ì´í„° í† í¬ë‚˜ì´ì§• ì¤‘...")

        # í•™ìŠµ ë°ì´í„° í† í¬ë‚˜ì´ì§•
        encoder_input_train_tokenized = tokenizer(
            encoder_input_train,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['encoder_max_len'],
            add_special_tokens=True
        )

        decoder_input_train_tokenized = tokenizer(
            decoder_input_train,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            add_special_tokens=False
        )

        decoder_output_train_tokenized = tokenizer(
            decoder_output_train,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            add_special_tokens=False
        )

        # ê²€ì¦ ë°ì´í„° í† í¬ë‚˜ì´ì§•
        encoder_input_val_tokenized = tokenizer(
            encoder_input_val,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['encoder_max_len'],
            add_special_tokens=True
        )

        decoder_input_val_tokenized = tokenizer(
            decoder_input_val,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            add_special_tokens=False
        )

        decoder_output_val_tokenized = tokenizer(
            decoder_output_val,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.config['tokenizer']['decoder_max_len'],
            add_special_tokens=False
        )

        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = DatasetForTrain(
            encoder_input_train_tokenized,
            decoder_input_train_tokenized,
            decoder_output_train_tokenized,
            len(encoder_input_train)
        )

        val_dataset = DatasetForVal(
            encoder_input_val_tokenized,
            decoder_input_val_tokenized,
            decoder_output_val_tokenized,
            len(encoder_input_val)
        )

        print(f"âœ… K-Fold ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: train={len(train_dataset)}, val={len(val_dataset)}")

        return train_dataset, val_dataset

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from config_manager import ConfigManager

    config_manager = ConfigManager()
    config = config_manager.load_config()

    processor = DataProcessor(config)
    print("ë°ì´í„° í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
