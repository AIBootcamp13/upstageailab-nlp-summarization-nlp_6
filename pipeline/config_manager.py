"""
ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ - ëª¨ë¸ë³„, í•˜ë“œì›¨ì–´ë³„ ìµœì í™” ì„¤ì • ê´€ë¦¬
"""

import yaml
import os
import glob
from typing import Dict, Any, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer


@dataclass
class HardwareSpec:
    """í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ì •ì˜"""
    name: str
    vram_gb: float
    compute_capability: float
    memory_bandwidth: int
    tensor_cores: bool = False


@dataclass
class ModelSpec:
    """ëª¨ë¸ ì‚¬ì–‘ ì •ì˜"""
    name: str
    model_id: str
    size: str  # 'small', 'base', 'large'
    parameters: int  # íŒŒë¼ë¯¸í„° ìˆ˜ (millions)
    recommended_vram: float
    supports_korean: bool = False
    supports_lora: bool = True  # LoRA ì§€ì› ì—¬ë¶€


@dataclass
class LoRAConfig:
    """LoRA ì„¤ì • ì •ì˜"""
    enabled: bool = False
    use_qlora: bool = False  # QLoRA ì‚¬ìš© ì—¬ë¶€
    r: int = 16  # LoRA rank
    alpha: int = 32  # LoRA alpha
    dropout: float = 0.1  # LoRA dropout
    target_modules: list = None  # íƒ€ê²Ÿ ëª¨ë“ˆë“¤
    bias: str = "none"  # bias ì„¤ì •
    task_type: str = "SEQ_2_SEQ_LM"  # íƒœìŠ¤í¬ íƒ€ì…


class ConfigPresets:
    """ì„¤ì • í”„ë¦¬ì…‹ ê´€ë¦¬ í´ë˜ìŠ¤"""

    # í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ì •ì˜
    HARDWARE_SPECS = {
        'rtx3060': HardwareSpec('RTX 3060', 6.0, 8.6, 360, True),
        'rtx3070': HardwareSpec('RTX 3070', 8.0, 8.6, 448, True),
        'rtx3080': HardwareSpec('RTX 3080', 10.0, 8.6, 760, True),
        'rtx3090': HardwareSpec('RTX 3090', 24.0, 8.6, 936, True),
        'rtx4070': HardwareSpec('RTX 4070', 12.0, 8.9, 504, True),
        'rtx4080': HardwareSpec('RTX 4080', 16.0, 8.9, 717, True),
        'rtx4090': HardwareSpec('RTX 4090', 24.0, 8.9, 1008, True),
        'cpu': HardwareSpec('CPU', 0.0, 0.0, 0, False),
    }

    # ëª¨ë¸ ì‚¬ì–‘ ì •ì˜
    MODEL_SPECS = {
        'kobart-base-v2': ModelSpec(
            'KoBART Base v2', 'gogamza/kobart-base-v2', 'base', 124, 4.0, True, True
        ),
        'kobart-summarization-large': ModelSpec(
            'KoBART Summarization', 'gogamza/kobart-summarization', 'large', 124, 6.0, True, True
        ),
        'bart-base': ModelSpec(
            'BART Base', 'facebook/bart-base', 'base', 139, 4.0, False, True
        ),
        'bart-large': ModelSpec(
            'BART Large', 'facebook/bart-large', 'large', 406, 8.0, False, True
        ),
        'kobart-summarization': ModelSpec(
            'KoBART Summarization', 'digit82/kobart-summarization', 'base', 124, 4.0, True, True
        ),
        # í° ëª¨ë¸ë“¤ (LoRA/QLoRA ì „ìš©)
        'bart-large-lora': ModelSpec(
            'BART Large (LoRA)', 'facebook/bart-large', 'large', 406, 4.0, False, True
        ),
        'kobart-large-lora': ModelSpec(
            'KoBART Large (LoRA)', 'gogamza/kobart-summarization', 'large', 124, 3.0, True, True
        ),
    }

    # LoRA ì„¤ì • í”„ë¦¬ì…‹
    LORA_PRESETS = {
        'disabled': LoRAConfig(enabled=False),
        'lora_light': LoRAConfig(
            enabled=True, use_qlora=False, r=8, alpha=16, dropout=0.05,
            target_modules=["q_proj", "v_proj", "k_proj", "out_proj"]
        ),
        'lora_standard': LoRAConfig(
            enabled=True, use_qlora=False, r=16, alpha=32, dropout=0.1,
            target_modules=["q_proj", "v_proj",
                            "k_proj", "out_proj", "fc1", "fc2"]
        ),
        'lora_heavy': LoRAConfig(
            enabled=True, use_qlora=False, r=32, alpha=64, dropout=0.1,
            target_modules=["q_proj", "v_proj",
                            "k_proj", "out_proj", "fc1", "fc2"]
        ),
        'qlora_standard': LoRAConfig(
            enabled=True, use_qlora=True, r=16, alpha=32, dropout=0.1,
            target_modules=["q_proj", "v_proj",
                            "k_proj", "out_proj", "fc1", "fc2"]
        ),
        'qlora_heavy': LoRAConfig(
            enabled=True, use_qlora=True, r=32, alpha=64, dropout=0.1,
            target_modules=["q_proj", "v_proj",
                            "k_proj", "out_proj", "fc1", "fc2"]
        ),
    }

    @staticmethod
    def get_optimal_model_for_hardware(hardware_key: str) -> str:
        """í•˜ë“œì›¨ì–´ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ"""
        hardware = ConfigPresets.HARDWARE_SPECS.get(hardware_key)
        if not hardware:
            return 'kobart-base-v2'

        # VRAM ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
        if hardware.vram_gb >= 12.0:
            return 'kobart-summarization-large'  # í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
        elif hardware.vram_gb >= 6.0:
            return 'kobart-base-v2'   # ì¤‘ê°„ ëª¨ë¸
        else:
            return 'kobart-base-v2'   # ì•ˆì „í•œ ì„ íƒ

    @staticmethod
    def get_optimal_lora_config(hardware_key: str, model_key: str) -> str:
        """í•˜ë“œì›¨ì–´ì™€ ëª¨ë¸ì— ìµœì í™”ëœ LoRA ì„¤ì • ì„ íƒ"""
        hardware = ConfigPresets.HARDWARE_SPECS.get(hardware_key)
        model = ConfigPresets.MODEL_SPECS.get(model_key)

        if not hardware or not model:
            return 'disabled'

        # í° ëª¨ë¸ì´ê±°ë‚˜ VRAMì´ ë¶€ì¡±í•œ ê²½ìš° LoRA ì‚¬ìš©
        if model.size == 'large' and hardware.vram_gb < 16.0:
            if hardware.vram_gb >= 8.0:
                return 'qlora_standard'  # QLoRAë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            else:
                return 'qlora_heavy'     # ë” ê°•í•œ QLoRA
        elif model.size == 'large' and hardware.vram_gb >= 16.0:
            return 'lora_standard'       # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ì—ì„œëŠ” ì¼ë°˜ LoRA
        elif hardware.vram_gb < 6.0:
            return 'lora_light'          # ì €ì‚¬ì–‘ì—ì„œëŠ” ê°€ë²¼ìš´ LoRA
        else:
            return 'disabled'            # ì¼ë°˜ í•™ìŠµ

    @staticmethod
    def get_training_config(hardware_key: str, model_key: str) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ì™€ ëª¨ë¸ì— ìµœì í™”ëœ í•™ìŠµ ì„¤ì •"""
        hardware = ConfigPresets.HARDWARE_SPECS.get(hardware_key)
        model = ConfigPresets.MODEL_SPECS.get(model_key)

        if not hardware or not model:
            return ConfigPresets._get_default_training_config()

        # ê¸°ë³¸ ì„¤ì •
        config = {
            "num_train_epochs": 6,
            "learning_rate": 3e-5,
            "weight_decay": 0.01,
            "warmup_ratio": 0.1,
            "eval_strategy": "steps",
            "save_strategy": "steps",
            "fp16": hardware.tensor_cores,
            "gradient_checkpointing": hardware.vram_gb < 12.0,
            "dataloader_pin_memory": hardware.vram_gb > 0,
            "save_total_limit": 3,
        }

        # í•˜ë“œì›¨ì–´ë³„ ìµœì í™”
        if hardware.vram_gb >= 20.0:  # RTX 3090, 4090
            config.update({
                "per_device_train_batch_size": 16 if model.size == 'base' else 12,
                "per_device_eval_batch_size": 16 if model.size == 'base' else 12,
                "gradient_accumulation_steps": 1,
                "dataloader_num_workers": 6,
                "eval_steps": 150,
                "save_steps": 150,
                "num_train_epochs": 8,
                "save_total_limit": 5,
            })
        elif hardware.vram_gb >= 12.0:  # RTX 4070, 3080
            config.update({
                "per_device_train_batch_size": 12 if model.size == 'base' else 8,
                "per_device_eval_batch_size": 12 if model.size == 'base' else 8,
                "gradient_accumulation_steps": 2,
                "dataloader_num_workers": 4,
                "eval_steps": 200,
                "save_steps": 200,
            })
        elif hardware.vram_gb >= 6.0:  # RTX 3060, 3070 - Final Score ìµœì í™”
            config.update({
                # ë°°ì¹˜ í¬ê¸° ì¡°ì • (ì•ˆì •ì„±)
                "per_device_train_batch_size": 12 if model.size == 'base' else 6,
                "per_device_eval_batch_size": 16 if model.size == 'base' else 8,   # í‰ê°€ëŠ” ë” í° ë°°ì¹˜
                "gradient_accumulation_steps": 3,  # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° = 12*3=36
                "dataloader_num_workers": 4,
                "eval_steps": 300,  # ë” ìì£¼ í‰ê°€í•˜ì—¬ ìµœì  ëª¨ë¸ ì°¾ê¸°
                "save_steps": 300,
                "num_train_epochs": 5,  # ì—í¬í¬ ì¦ê°€ë¡œ ë” ë‚˜ì€ ìˆ˜ë ´
                "gradient_checkpointing": True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ìœ¼ë¡œ ë” í° ëª¨ë¸ í•™ìŠµ
                "learning_rate": 2e-5,  # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ
                "warmup_ratio": 0.15,  # ë” ê¸´ ì›Œë°ì—…ìœ¼ë¡œ ì•ˆì •ì  ì‹œì‘
                "weight_decay": 0.02,  # ì •ê·œí™” ê°•í™”
                "lr_scheduler_type": "cosine_with_restarts",  # ë” ë‚˜ì€ ìŠ¤ì¼€ì¤„ëŸ¬
                "cosine_restarts": 2,  # ì¬ì‹œì‘ìœ¼ë¡œ local minima íƒˆì¶œ
            })
        else:  # CPU
            config.update({
                "per_device_train_batch_size": 2,
                "per_device_eval_batch_size": 2,
                "gradient_accumulation_steps": 16,
                "dataloader_num_workers": 0,
                "dataloader_pin_memory": False,
                "fp16": False,
                "eval_strategy": "epoch",
                "save_strategy": "epoch",
                "num_train_epochs": 3,
            })

        # ëª¨ë¸ë³„ í•™ìŠµë¥  ì¡°ì •
        if model.size == 'large':
            config["learning_rate"] = 2e-5  # í° ëª¨ë¸ì€ ë‚®ì€ í•™ìŠµë¥ 
        elif model.size == 'small':
            config["learning_rate"] = 5e-5  # ì‘ì€ ëª¨ë¸ì€ ë†’ì€ í•™ìŠµë¥ 

        return config

    @staticmethod
    def get_tokenizer_config(hardware_key: str, model_key: str) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ì™€ ëª¨ë¸ì— ìµœì í™”ëœ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        hardware = ConfigPresets.HARDWARE_SPECS.get(hardware_key)
        model = ConfigPresets.MODEL_SPECS.get(model_key)

        # ê¸°ë³¸ ì„¤ì •
        config = {
            "encoder_max_len": 256,
            "decoder_max_len": 64,
        }

        # í•˜ë“œì›¨ì–´ë³„ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • (RTX 3060 ìµœì í™”)
        if hardware and hardware.vram_gb >= 20.0:
            config.update({
                "encoder_max_len": 768 if model and model.size == 'large' else 512,
                "decoder_max_len": 192 if model and model.size == 'large' else 128,
            })
        elif hardware and hardware.vram_gb >= 12.0:
            config.update({
                "encoder_max_len": 512,
                "decoder_max_len": 128,
            })
        elif hardware and hardware.vram_gb >= 6.0:  # RTX 3060 Final Score ìµœì í™”
            config.update({
                "encoder_max_len": 384,  # ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì •ë³´ ë³´ì¡´
                "decoder_max_len": 96,   # ë” ê¸´ ì¶œë ¥ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ
            })
        elif hardware and hardware.vram_gb < 6.0:  # CPU
            config.update({
                "encoder_max_len": 128,
                "decoder_max_len": 32,
            })

        return config

    @staticmethod
    def get_inference_config(hardware_key: str, model_key: str) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ì™€ ëª¨ë¸ì— ìµœì í™”ëœ ì¸í¼ëŸ°ìŠ¤ ì„¤ì •"""
        hardware = ConfigPresets.HARDWARE_SPECS.get(hardware_key)

        # ê¸°ë³¸ ì„¤ì •
        config = {
            "batch_size": 32,
            "num_beams": 4,
            "no_repeat_ngram_size": 2,
            "early_stopping": True,
            "length_penalty": 1.2,
            "repetition_penalty": 1.1,
        }

        # í•˜ë“œì›¨ì–´ë³„ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if hardware and hardware.vram_gb >= 20.0:
            config.update({
                "batch_size": 96,
                "num_beams": 6,
                "no_repeat_ngram_size": 4,
                "length_penalty": 1.3,
                "repetition_penalty": 1.15,
            })
        elif hardware and hardware.vram_gb >= 12.0:
            config.update({
                "batch_size": 64,
                "num_beams": 5,
                "no_repeat_ngram_size": 3,
            })
        elif hardware and hardware.vram_gb < 6.0:  # CPU
            config.update({
                "batch_size": 4,
                "num_beams": 3,
            })

        return config

    @staticmethod
    def _get_default_training_config() -> Dict[str, Any]:
        """ê¸°ë³¸ í•™ìŠµ ì„¤ì •"""
        return {
            "num_train_epochs": 6,
            "learning_rate": 3e-5,
            "per_device_train_batch_size": 8,
            "per_device_eval_batch_size": 8,
            "gradient_accumulation_steps": 4,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "eval_strategy": "steps",
            "save_strategy": "steps",
            "eval_steps": 300,
            "save_steps": 300,
            "fp16": True,
            "gradient_checkpointing": True,
            "dataloader_num_workers": 2,
            "dataloader_pin_memory": True,
            "save_total_limit": 3,
        }


class ConfigManager:
    def __init__(self, config_path="config.yaml"):
        self.config_path = config_path
        self.config = None
        self.presets = ConfigPresets()

    @staticmethod
    def find_latest_augmented_data_path(base_path: str = "./data") -> str:
        """ìµœì‹  ì¦ê°• ë°ì´í„° í´ë” ìë™ íƒì§€"""
        try:
            # augmented_ë¡œ ì‹œì‘í•˜ëŠ” í´ë”ë“¤ ì°¾ê¸°
            pattern = os.path.join(base_path, "augmented_*")
            augmented_folders = glob.glob(pattern)

            if not augmented_folders:
                print(f"âš ï¸ {base_path}ì—ì„œ ì¦ê°• ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return base_path

            # í´ë”ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œí•˜ì—¬ ì •ë ¬
            def extract_timestamp(folder_path):
                folder_name = os.path.basename(folder_path)
                # augmented_method_path_ratio_timestamp í˜•ì‹ì—ì„œ timestamp ì¶”ì¶œ
                parts = folder_name.split('_')
                if len(parts) >= 5:
                    # ë§ˆì§€ë§‰ ë‘ ë¶€ë¶„ì´ ë‚ ì§œì™€ ì‹œê°„ (YYYYMMDD_HHMMSS)
                    try:
                        timestamp = f"{parts[-2]}_{parts[-1]}"
                        return timestamp
                    except:
                        pass
                return "00000000_000000"  # ê¸°ë³¸ê°’

            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹  ìˆœ)
            augmented_folders.sort(key=extract_timestamp, reverse=True)
            latest_folder = augmented_folders[0]

            print(f"ğŸ” ìµœì‹  ì¦ê°• ë°ì´í„° í´ë” ê°ì§€: {os.path.basename(latest_folder)}")
            return latest_folder

        except Exception as e:
            print(f"âš ï¸ ì¦ê°• ë°ì´í„° í´ë” íƒì§€ ì‹¤íŒ¨: {e}")
            return base_path

    def create_optimized_config(self,
                                hardware_key: Optional[str] = None,
                                model_key: Optional[str] = None,
                                data_path: Optional[str] = None) -> Dict[str, Any]:
        """í•˜ë“œì›¨ì–´ì™€ ëª¨ë¸ì— ìµœì í™”ëœ ì„¤ì • ìƒì„±"""

        # ë°ì´í„° ê²½ë¡œ ìë™ ê°ì§€
        if data_path is None:
            data_path = self.find_latest_augmented_data_path()

        # í•˜ë“œì›¨ì–´ ìë™ ê°ì§€
        if hardware_key is None:
            hardware_key = self._detect_hardware()

        # ëª¨ë¸ ìë™ ì„ íƒ
        if model_key is None:
            model_key = self.presets.get_optimal_model_for_hardware(
                hardware_key)

        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_spec = self.presets.MODEL_SPECS.get(model_key)
        if not model_spec:
            raise ValueError(f"Unknown model key: {model_key}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_spec.model_id)
        except Exception as e:
            print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-base-v2")

        # LoRA ì„¤ì • ê²°ì •
        lora_preset_key = self.presets.get_optimal_lora_config(
            hardware_key, model_key)
        lora_config = self.presets.LORA_PRESETS.get(lora_preset_key)

        # ì„¤ì • êµ¬ì„±
        config_data = {
            "general": {
                "data_path": data_path,
                "model_name": model_spec.model_id,
                "output_dir": "./model_output/",
                "hardware": hardware_key,
                "model_key": model_key,
                "lora_preset": lora_preset_key,
            },
            "lora": {
                "enabled": lora_config.enabled,
                "use_qlora": lora_config.use_qlora,
                "r": lora_config.r,
                "alpha": lora_config.alpha,
                "dropout": lora_config.dropout,
                "target_modules": lora_config.target_modules or ["q_proj", "v_proj"],
                "bias": lora_config.bias,
                "task_type": lora_config.task_type,
            },
            "tokenizer": {
                **self.presets.get_tokenizer_config(hardware_key, model_key),
                "bos_token": str(tokenizer.bos_token),
                "eos_token": str(tokenizer.eos_token),
                "special_tokens": [
                    'A:', 'B:',
                    '#PhoneNumber#', '#Address#', '#DateOfBirth#',
                    '#PassportNumber#', '#SSN#', '#CardNumber#',
                    '#CarNumber#', '#Email#'
                ]
            },
            "training": {
                **self.presets.get_training_config(hardware_key, model_key),
                "overwrite_output_dir": True,
                "load_best_model_at_end": True,
                "metric_for_best_model": "eval_final_score",
                "greater_is_better": True,
                "seed": 42,
                "logging_dir": "./logs",
                "logging_strategy": "steps",
                "logging_steps": 50,
                "predict_with_generate": True,
                "generation_max_length": self.presets.get_tokenizer_config(hardware_key, model_key)["decoder_max_len"],
                "do_train": True,
                "do_eval": True,
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.001,
                "report_to": "none",
                "lr_scheduler_type": "cosine",
                "optim": "adamw_torch",
            },
            "inference": {
                **self.presets.get_inference_config(hardware_key, model_key),
                "ckt_path": "./model_output/",
                "result_path": "./prediction/",
                "generate_max_length": self.presets.get_tokenizer_config(hardware_key, model_key)["decoder_max_len"],
                "remove_tokens": ['<usr>', str(tokenizer.bos_token), str(tokenizer.eos_token), str(tokenizer.pad_token)]
            },
            "wandb": {
                "entity": "your_entity",
                "project": "dialogue_summarization",
                "name": f"{model_key}_{hardware_key}_run"
            }
        }

        return config_data

    def _detect_hardware(self) -> str:
        """í•˜ë“œì›¨ì–´ ìë™ ê°ì§€"""
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0).lower()
                gpu_memory = torch.cuda.get_device_properties(
                    0).total_memory / (1024**3)

                print(f"ğŸ–¥ï¸ ê°ì§€ëœ GPU: {torch.cuda.get_device_name(0)}")
                print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")

                # GPU ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­
                for key in self.presets.HARDWARE_SPECS.keys():
                    if key.replace('rtx', 'rtx ') in gpu_name:
                        return key

                # ë©”ëª¨ë¦¬ ê¸°ë°˜ ë§¤ì¹­
                if gpu_memory >= 22:
                    return 'rtx4090'
                elif gpu_memory >= 20:
                    return 'rtx3090'
                elif gpu_memory >= 14:
                    return 'rtx4080'
                elif gpu_memory >= 10:
                    return 'rtx4070'
                elif gpu_memory >= 8:
                    return 'rtx3070'
                else:
                    return 'rtx3060'
            else:
                return 'cpu'
        except Exception as e:
            print(f"âš ï¸ í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return 'rtx3060'  # ê¸°ë³¸ê°’

    def create_default_config(self, model_name="digit82/kobart-summarization", data_path=None):
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        # ë°ì´í„° ê²½ë¡œ ìë™ ê°ì§€
        if data_path is None:
            data_path = self.find_latest_augmented_data_path()

        tokenizer = AutoTokenizer.from_pretrained(model_name)

        config_data = {
            "general": {
                "data_path": data_path,
                "model_name": model_name,
                "output_dir": "./model_output/"
            },
            "tokenizer": {
                "encoder_max_len": 512,
                "decoder_max_len": 100,
                "bos_token": f"{tokenizer.bos_token}",
                "eos_token": f"{tokenizer.eos_token}",
                "special_tokens": [
                    'A:', 'B:',
                    '#PhoneNumber#', '#Address#', '#DateOfBirth#',
                    '#PassportNumber#', '#SSN#', '#CardNumber#',
                    '#CarNumber#', '#Email#'
                ]
            },
            "training": {
                "overwrite_output_dir": True,
                "num_train_epochs": 10,
                "learning_rate": 1e-5,
                "per_device_train_batch_size": 16,
                "per_device_eval_batch_size": 16,
                "warmup_ratio": 0.1,
                "weight_decay": 0.01,
                "lr_scheduler_type": 'cosine',
                "optim": 'adamw_torch',
                "gradient_accumulation_steps": 2,
                "evaluation_strategy": 'epoch',
                "save_strategy": 'epoch',
                "save_total_limit": 3,
                "fp16": True,
                "load_best_model_at_end": True,
                "metric_for_best_model": "eval_final_score",
                "greater_is_better": True,
                "seed": 42,
                "logging_dir": "./logs",
                "logging_strategy": "epoch",
                "predict_with_generate": True,
                "generation_max_length": 100,
                "do_train": True,
                "do_eval": True,
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.001,
                "report_to": "none"
            },
            "wandb": {
                "entity": "your_entity",
                "project": "dialogue_summarization",
                "name": "baseline_run"
            },
            "inference": {
                "ckt_path": "./model_output/",
                "result_path": "./prediction/",
                "no_repeat_ngram_size": 2,
                "early_stopping": True,
                "generate_max_length": 100,
                "num_beams": 4,
                "batch_size": 32,
                "remove_tokens": ['<usr>', f"{tokenizer.bos_token}", f"{tokenizer.eos_token}", f"{tokenizer.pad_token}"]
            },
            "kfold": {
                "enabled": False,
                "n_splits": 5,
                "stratified": False,
                "random_state": 42,
                "ensemble_method": "voting",  # voting, weighted, best
                "save_individual_models": True,
                "use_ensemble_inference": True
            }
        }

        return config_data

    def save_config(self, config_data):
        """ì„¤ì •ì„ YAML íŒŒì¼ë¡œ ì €ì¥"""
        with open(self.config_path, "w", encoding='utf-8') as file:
            yaml.dump(config_data, file, allow_unicode=True,
                      default_flow_style=False)
        print(f"ì„¤ì •ì´ {self.config_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def load_config(self):
        """YAML íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
        if not os.path.exists(self.config_path):
            print(f"{self.config_path}ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            config_data = self.create_default_config()
            self.save_config(config_data)

        with open(self.config_path, "r", encoding='utf-8') as file:
            self.config = yaml.safe_load(file)

        return self.config

    def update_config(self, updates):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        if self.config is None:
            self.load_config()

        def deep_update(base_dict, update_dict):
            for key, value in update_dict.items():
                if isinstance(value, dict) and key in base_dict:
                    deep_update(base_dict[key], value)
                else:
                    base_dict[key] = value

        deep_update(self.config, updates)
        self.save_config(self.config)

        return self.config

    def get_config(self):
        """í˜„ì¬ ì„¤ì • ë°˜í™˜"""
        if self.config is None:
            self.load_config()
        return self.config

    def print_config(self):
        """ì„¤ì • ì¶œë ¥"""
        if self.config is None:
            self.load_config()

        print("=== í˜„ì¬ ì„¤ì • ===")
        for section, values in self.config.items():
            print(f"\n[{section}]")
            if isinstance(values, dict):
                for key, value in values.items():
                    print(f"  {key}: {value}")
            else:
                print(f"  {values}")

# í¸ì˜ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)


def create_optimized_config_for_rtx3060():
    """RTX 3060ì— ìµœì í™”ëœ ì„¤ì • ìƒì„± - Final Score í–¥ìƒ ë²„ì „"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # Final Score í–¥ìƒì„ ìœ„í•œ ìµœì í™”
    config['training'].update({
        # í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
        "learning_rate": 2e-5,              # ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  ìˆ˜ë ´
        "per_device_train_batch_size": 12,  # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°
        "gradient_accumulation_steps": 3,   # íš¨ê³¼ì  ë°°ì¹˜ = 36
        "weight_decay": 0.02,               # ê°•í™”ëœ ì •ê·œí™”
        "max_grad_norm": 0.5,               # ê°•í•œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        "warmup_ratio": 0.15,               # ê¸´ ì›Œë°ì—…

        # í•™ìŠµ ì¼ì • ìµœì í™”
        "num_train_epochs": 5,              # ì¶©ë¶„í•œ í•™ìŠµ
        "eval_steps": 300,                  # ìì£¼ í‰ê°€
        "save_steps": 300,
        "early_stopping_patience": 5,      # ê¸´ patience
        "early_stopping_threshold": 0.005,  # ë¯¼ê°í•œ threshold

        # ìƒì„± í’ˆì§ˆ í–¥ìƒ
        "generation_num_beams": 5,          # ë” ë§ì€ beam
        "generation_length_penalty": 1.1,  # ì ì ˆí•œ penalty
        "generation_no_repeat_ngram_size": 3,  # ë°˜ë³µ ë°©ì§€ ê°•í™”

        # ìµœì í™”ê¸° ì„¤ì •
        "adam_epsilon": 1e-6,               # ì•ˆì •ì„± í–¥ìƒ
        "adam_beta1": 0.9,
        "adam_beta2": 0.98,                 # ë” ë‚˜ì€ ìˆ˜ë ´

        # ê¸°íƒ€ ìµœì í™”
        "dataloader_drop_last": False,
        "logging_steps": 25,
        "save_total_limit": 5,
        "lr_scheduler_type": "cosine_with_restarts",
        "cosine_restarts": 2,
    })

    # í† í¬ë‚˜ì´ì € í’ˆì§ˆ í–¥ìƒ
    config['tokenizer'].update({
        "encoder_max_len": 384,             # ê¸´ ì»¨í…ìŠ¤íŠ¸
        "decoder_max_len": 96,              # ê¸´ ì¶œë ¥
        "padding": "max_length",
        "truncation": True,
        "return_attention_mask": True,
    })

    # ì¶”ë¡  í’ˆì§ˆ í–¥ìƒ
    config['inference'].update({
        "num_beams": 5,
        "length_penalty": 1.1,
        "repetition_penalty": 1.05,
        "no_repeat_ngram_size": 3,
        "early_stopping": True,
        "do_sample": False,
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_high_quality_config_for_rtx3060():
    """RTX 3060ìš© ê³ í’ˆì§ˆ Final Score ìµœì í™” ì„¤ì •"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # ê³ í’ˆì§ˆ í•™ìŠµì„ ìœ„í•œ ì„¤ì •
    config['training'].update({
        # í•™ìŠµ íŒŒë¼ë¯¸í„° ìµœì í™”
        'num_train_epochs': 8,              # ì¶©ë¶„í•œ í•™ìŠµ
        'learning_rate': 1.5e-5,            # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ 
        'per_device_train_batch_size': 8,   # ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì •ì„±
        'per_device_eval_batch_size': 12,
        'gradient_accumulation_steps': 6,   # íš¨ê³¼ì  ë°°ì¹˜ = 48

        # ìŠ¤ì¼€ì¤„ë§ ìµœì í™”
        'warmup_ratio': 0.2,                # ê¸´ ì›Œë°ì—…
        'lr_scheduler_type': 'polynomial',  # ë¶€ë“œëŸ¬ìš´ ê°ì†Œ
        'polynomial_decay_power': 2.0,

        # ì •ê·œí™” ê°•í™”
        'weight_decay': 0.05,               # ê°•í•œ ì •ê·œí™”
        'max_grad_norm': 0.3,               # ê°•í•œ í´ë¦¬í•‘
        'label_smoothing_factor': 0.1,      # ë¼ë²¨ ìŠ¤ë¬´ë”©

        # í‰ê°€ ë° ì €ì¥ ìµœì í™”
        'eval_steps': 200,                  # ìì£¼ í‰ê°€
        'save_steps': 200,
        'early_stopping_patience': 8,      # ë§¤ìš° ê¸´ patience
        'early_stopping_threshold': 0.001,

        # ìƒì„± í’ˆì§ˆ ìµœëŒ€í™”
        'generation_num_beams': 8,          # ìµœëŒ€ beam
        'generation_length_penalty': 1.2,
        'generation_no_repeat_ngram_size': 4,
        'generation_do_sample': False,

        # ë©”ëª¨ë¦¬ ìµœì í™”
        'gradient_checkpointing': True,
        'dataloader_pin_memory': True,
        'dataloader_num_workers': 2,

        # ê¸°íƒ€ ìµœì í™”
        'fp16': True,
        'dataloader_drop_last': False,
        'ignore_data_skip': True,
    })

    # í† í¬ë‚˜ì´ì € ê³ í’ˆì§ˆ ì„¤ì •
    config['tokenizer'].update({
        'encoder_max_len': 512,             # ê¸´ ì»¨í…ìŠ¤íŠ¸
        'decoder_max_len': 128,             # ê¸´ ì¶œë ¥
        'padding': 'max_length',
        'truncation': True,
        'return_attention_mask': True,
    })

    # ì¶”ë¡  ê³ í’ˆì§ˆ ì„¤ì •
    config['inference'].update({
        'num_beams': 8,
        'length_penalty': 1.2,
        'repetition_penalty': 1.02,
        'no_repeat_ngram_size': 4,
        'early_stopping': True,
        'do_sample': False,
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_balanced_config_for_rtx3060():
    """RTX 3060ìš© ì†ë„ì™€ í’ˆì§ˆì˜ ê· í˜• ì¡íŒ ì„¤ì •"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # ê· í˜• ì¡íŒ ì„¤ì •
    config['training'].update({
        'num_train_epochs': 6,
        'learning_rate': 2.5e-5,
        'per_device_train_batch_size': 10,
        'per_device_eval_batch_size': 14,
        'gradient_accumulation_steps': 4,   # íš¨ê³¼ì  ë°°ì¹˜ = 40

        'warmup_ratio': 0.12,
        'weight_decay': 0.03,
        'max_grad_norm': 0.4,

        'eval_steps': 250,
        'save_steps': 250,
        'early_stopping_patience': 6,

        'generation_num_beams': 6,
        'generation_length_penalty': 1.15,
        'generation_no_repeat_ngram_size': 3,

        'gradient_checkpointing': True,
        'dataloader_num_workers': 3,
    })

    config['tokenizer'].update({
        'encoder_max_len': 384,
        'decoder_max_len': 96,
    })

    config['inference'].update({
        'num_beams': 6,
        'length_penalty': 1.15,
        'repetition_penalty': 1.03,
        'no_repeat_ngram_size': 3,
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_optimized_config_for_rtx3090():
    """RTX 3090ì— ìµœì í™”ëœ ì„¤ì • ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3090', 'kobart-summarization-large')
    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_rtx3090_baseline_config():
    """RTX 3090ìš© Baseline ì„¤ì • ìƒì„± (baseline.ipynb ê¸°ë°˜)"""
    from .rtx3090_baseline_config import create_rtx3090_baseline_config as _create_baseline
    return _create_baseline()


def create_rtx3090_baseline_kfold_config(n_splits=5, ensemble_method='voting'):
    """RTX 3090ìš© Baseline + K-Fold ì„¤ì •"""
    from .rtx3090_baseline_config import create_rtx3090_baseline_kfold_config as _create_kfold
    return _create_kfold(n_splits, ensemble_method)


def create_rtx3090_baseline_fast_config():
    """RTX 3090ìš© ë¹ ë¥¸ Baseline ì„¤ì • (ì‹¤í—˜ìš©)"""
    from .rtx3090_baseline_config import create_rtx3090_baseline_fast_config as _create_fast
    return _create_fast()


def create_optimized_config_for_rtx4090():
    """RTX 4090ì— ìµœì í™”ëœ ì„¤ì • ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx4090', 'kobart-summarization-large')
    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_auto_optimized_config():
    """GPUë¥¼ ìë™ ê°ì§€í•˜ì—¬ ìµœì í™”ëœ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config()
    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_custom_config(hardware: str, model: str, data_path: Optional[str] = None, **kwargs):
    """ì»¤ìŠ¤í…€ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        hardware, model, data_path, **kwargs)
    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_lora_config(hardware: str, model: str, lora_type: str = 'auto', data_path: Optional[str] = None, **kwargs):
    """LoRA ì„¤ì •ìœ¼ë¡œ ì»¤ìŠ¤í…€ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        hardware, model, data_path, **kwargs)

    # LoRA ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    if lora_type != 'auto':
        lora_preset = config_manager.presets.LORA_PRESETS.get(lora_type)
        if lora_preset:
            config['lora'] = {
                "enabled": lora_preset.enabled,
                "use_qlora": lora_preset.use_qlora,
                "r": lora_preset.r,
                "alpha": lora_preset.alpha,
                "dropout": lora_preset.dropout,
                "target_modules": lora_preset.target_modules or ["q_proj", "v_proj"],
                "bias": lora_preset.bias,
                "task_type": lora_preset.task_type,
            }
            config['general']['lora_preset'] = lora_type

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_qlora_config(hardware: str, model: str = 'bart-large', data_path: Optional[str] = None, **kwargs):
    """QLoRA ì„¤ì •ìœ¼ë¡œ í° ëª¨ë¸ í•™ìŠµ ì„¤ì • ìƒì„±"""
    return create_lora_config(hardware, model, 'qlora_standard', data_path, **kwargs)


def create_fast_config_for_rtx3060():
    """RTX 3060ìš© ì´ˆê³ ì† í•™ìŠµ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # ì´ˆê³ ì† í•™ìŠµ ì„¤ì •
    config['training'].update({
        'num_train_epochs': 2,  # ë§¤ìš° ì ì€ ì—í¬í¬
        'learning_rate': 5e-5,  # ë†’ì€ í•™ìŠµë¥ ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
        'per_device_train_batch_size': 24,  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
        'per_device_eval_batch_size': 24,
        'gradient_accumulation_steps': 1,  # accumulation ì œê±°
        'eval_steps': 1000,  # í‰ê°€ ë¹ˆë„ ëŒ€í­ ê°ì†Œ
        'save_steps': 1000,
        'gradient_checkpointing': False,  # ì†ë„ ìš°ì„ 
        'dataloader_num_workers': 6,  # ìµœëŒ€ ì›Œì»¤
        'fp16': True,  # Mixed precision ê°•ì œ í™œì„±í™”
    })

    # ì§§ì€ ì‹œí€€ìŠ¤ë¡œ ì†ë„ í–¥ìƒ
    config['tokenizer'].update({
        'encoder_max_len': 128,  # ë§¤ìš° ì§§ì€ ì…ë ¥
        'decoder_max_len': 32,   # ë§¤ìš° ì§§ì€ ì¶œë ¥
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_high_quality_config_for_rtx3060():
    """RTX 3060ìš© ê³ í’ˆì§ˆ Final Score ìµœì í™” ì„¤ì •"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # ê³ í’ˆì§ˆ í•™ìŠµì„ ìœ„í•œ ì„¤ì •
    config['training'].update({
        # í•™ìŠµ íŒŒë¼ë¯¸í„° ìµœì í™”
        'num_train_epochs': 8,  # ì¶©ë¶„í•œ í•™ìŠµ
        'learning_rate': 1.5e-5,  # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ë¡œ ì •ë°€ í•™ìŠµ
        'per_device_train_batch_size': 8,  # ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì •ì„±
        'per_device_eval_batch_size': 12,
        'gradient_accumulation_steps': 6,  # íš¨ê³¼ì  ë°°ì¹˜ = 8*6=48

        # ìŠ¤ì¼€ì¤„ë§ ìµœì í™”
        'warmup_ratio': 0.2,  # ê¸´ ì›Œë°ì—…
        'lr_scheduler_type': 'polynomial',  # ë¶€ë“œëŸ¬ìš´ ê°ì†Œ
        'polynomial_decay_power': 2.0,

        # ì •ê·œí™” ê°•í™”
        'weight_decay': 0.05,  # ê°•í•œ ì •ê·œí™”
        'max_grad_norm': 0.3,  # ê°•í•œ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        'label_smoothing_factor': 0.1,  # ë¼ë²¨ ìŠ¤ë¬´ë”©

        # í‰ê°€ ë° ì €ì¥ ìµœì í™”
        'eval_steps': 200,  # ìì£¼ í‰ê°€
        'save_steps': 200,
        'early_stopping_patience': 8,  # ë§¤ìš° ê¸´ patience
        'early_stopping_threshold': 0.001,

        # ìƒì„± í’ˆì§ˆ ìµœëŒ€í™”
        'generation_num_beams': 8,  # ìµœëŒ€ beam
        'generation_length_penalty': 1.2,
        'generation_no_repeat_ngram_size': 4,
        'generation_do_sample': False,  # ê²°ì •ì  ìƒì„±

        # ë©”ëª¨ë¦¬ ìµœì í™”
        'gradient_checkpointing': True,
        'dataloader_pin_memory': True,
        'dataloader_num_workers': 2,  # ì•ˆì •ì„± ìš°ì„ 

        # ê¸°íƒ€ ìµœì í™”
        'fp16': True,
        'fp16_opt_level': 'O1',  # ì•ˆì •ì ì¸ mixed precision
        'dataloader_drop_last': False,
        'ignore_data_skip': True,
    })

    # í† í¬ë‚˜ì´ì € ê³ í’ˆì§ˆ ì„¤ì •
    config['tokenizer'].update({
        'encoder_max_len': 512,  # ê¸´ ì»¨í…ìŠ¤íŠ¸
        'decoder_max_len': 128,  # ê¸´ ì¶œë ¥
        'padding': 'max_length',
        'truncation': True,
        'return_attention_mask': True,
        'return_token_type_ids': False,
    })

    # ì¶”ë¡  ê³ í’ˆì§ˆ ì„¤ì •
    config['inference'].update({
        'num_beams': 8,
        'length_penalty': 1.2,
        'repetition_penalty': 1.02,
        'no_repeat_ngram_size': 4,
        'early_stopping': True,
        'do_sample': False,
        'temperature': 1.0,
        'top_k': 50,
        'top_p': 1.0,
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


def create_balanced_config_for_rtx3060():
    """RTX 3060ìš© ì†ë„ì™€ í’ˆì§ˆì˜ ê· í˜• ì¡íŒ ì„¤ì •"""
    config_manager = ConfigManager()
    config = config_manager.create_optimized_config(
        'rtx3060', 'kobart-base-v2')

    # ê· í˜• ì¡íŒ ì„¤ì •
    config['training'].update({
        'num_train_epochs': 6,
        'learning_rate': 2.5e-5,
        'per_device_train_batch_size': 10,
        'per_device_eval_batch_size': 14,
        'gradient_accumulation_steps': 4,  # íš¨ê³¼ì  ë°°ì¹˜ = 40

        'warmup_ratio': 0.12,
        'weight_decay': 0.03,
        'max_grad_norm': 0.4,

        'eval_steps': 250,
        'save_steps': 250,
        'early_stopping_patience': 6,

        'generation_num_beams': 6,
        'generation_length_penalty': 1.15,
        'generation_no_repeat_ngram_size': 3,

        'gradient_checkpointing': True,
        'dataloader_num_workers': 3,
    })

    config['tokenizer'].update({
        'encoder_max_len': 384,
        'decoder_max_len': 96,
    })

    config['inference'].update({
        'num_beams': 6,
        'length_penalty': 1.15,
        'repetition_penalty': 1.03,
        'no_repeat_ngram_size': 3,
    })

    config_manager.config = config
    config_manager.save_config(config)
    return config


if __name__ == "__main__":
    print("ğŸ§ª ìƒˆë¡œìš´ ConfigManager í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    config_manager = ConfigManager()

    # 1. ìë™ ìµœì í™” ì„¤ì •
    print("\n1. ğŸš€ ìë™ í•˜ë“œì›¨ì–´ ê°ì§€ ë° ìµœì í™”:")
    auto_config = create_auto_optimized_config()

    # 2. ì»¤ìŠ¤í…€ ì„¤ì • í…ŒìŠ¤íŠ¸
    print("\n2. ğŸ¯ ì»¤ìŠ¤í…€ ì„¤ì • í…ŒìŠ¤íŠ¸:")

    # ë‹¤ì–‘í•œ ì¡°í•© í…ŒìŠ¤íŠ¸
    test_combinations = [
        ('rtx3060', 'kobart-base-v2'),
        ('rtx3090', 'kobart-summarization-large'),
        ('rtx4090', 'bart-large'),
        ('cpu', 'kobart-base-v2'),
    ]

    for hardware, model in test_combinations:
        print(f"\n   {hardware.upper()} + {model}:")
        try:
            config = create_custom_config(hardware, model)
            training_config = config['training']
            tokenizer_config = config['tokenizer']

            print(
                f"     ë°°ì¹˜ í¬ê¸°: {training_config['per_device_train_batch_size']}")
            print(f"     í•™ìŠµë¥ : {training_config['learning_rate']}")
            print(
                f"     ì‹œí€€ìŠ¤ ê¸¸ì´: {tokenizer_config['encoder_max_len']}/{tokenizer_config['decoder_max_len']}")
            print(f"     ì—í¬í¬: {training_config['num_train_epochs']}")
            print(f"     FP16: {training_config['fp16']}")
            print(
                f"     Gradient Checkpointing: {training_config['gradient_checkpointing']}")
        except Exception as e:
            print(f"     âŒ ì˜¤ë¥˜: {e}")

    # 3. í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ì •ë³´
    print("\n3. ğŸ“Š ì§€ì›ë˜ëŠ” í•˜ë“œì›¨ì–´ ì‚¬ì–‘:")
    for key, spec in ConfigPresets.HARDWARE_SPECS.items():
        print(f"   {key}: {spec.name} ({spec.vram_gb}GB VRAM)")

    # 4. ëª¨ë¸ ì‚¬ì–‘ ì •ë³´
    print("\n4. ğŸ¤– ì§€ì›ë˜ëŠ” ëª¨ë¸ ì‚¬ì–‘:")
    for key, spec in ConfigPresets.MODEL_SPECS.items():
        print(
            f"   {key}: {spec.name} ({spec.parameters}M params, {spec.recommended_vram}GB ê¶Œì¥)")

    print("\nâœ… ìƒˆë¡œìš´ ConfigManager í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:")
    print("   # ìë™ ìµœì í™”")
    print("   config = create_auto_optimized_config()")
    print("   # ì»¤ìŠ¤í…€ ì„¤ì •")
    print("   config = create_custom_config('rtx3090', 'kobart-summarization-large')")
    print("   # íŠ¹ì • GPU ì„¤ì •")
    print("   config = create_optimized_config_for_rtx4090()")


def create_kfold_config_for_rtx3060(n_splits=5, ensemble_method='voting'):
    """RTX 3060ìš© K-Fold êµì°¨ ê²€ì¦ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()

    # ê¸°ë³¸ ê³ í’ˆì§ˆ ì„¤ì •ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
    config = create_high_quality_config_for_rtx3060()

    # K-Fold ì„¤ì • í™œì„±í™” ë° ì¡°ì •
    config['kfold'] = {
        'enabled': True,
        'n_splits': n_splits,
        'stratified': True,  # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê³„ì¸µí™”
        'random_state': 42,
        'ensemble_method': ensemble_method,
        'save_individual_models': True,
        'use_ensemble_inference': True
    }

    # K-Foldì— ìµœì í™”ëœ í•™ìŠµ ì„¤ì • ì¡°ì •
    config['training'].update({
        'num_train_epochs': 8,  # foldë³„ë¡œ ì¡°ê¸ˆ ì¤„ì„
        'early_stopping_patience': 2,  # ë” ë¹ ë¥¸ ì¡°ê¸° ì¢…ë£Œ
        'save_total_limit': 2,  # ì €ì¥ ê³µê°„ ì ˆì•½
        'evaluation_strategy': 'steps',
        'eval_steps': 100,
        'save_steps': 100,
        'logging_steps': 50,
        'metric_for_best_model': 'eval_final_score',
        'load_best_model_at_end': True,
        'greater_is_better': True
    })

    # ë©”ëª¨ë¦¬ ìµœì í™” (ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´)
    config['training'].update({
        'per_device_train_batch_size': 8,  # ë°°ì¹˜ í¬ê¸° ì¤„ì„
        'per_device_eval_batch_size': 8,
        'gradient_accumulation_steps': 4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ ë³´ìƒ
        'dataloader_num_workers': 2,
        'dataloader_pin_memory': False
    })

    # LoRA ì„¤ì • ìµœì í™”
    if 'lora' in config:
        config['lora'].update({
            'r': 16,  # ì ì ˆí•œ rank
            'alpha': 32,
            'dropout': 0.1,
            'target_modules': ['q_proj', 'v_proj', 'k_proj', 'out_proj', 'fc1', 'fc2']
        })

    print("ğŸ”„ RTX 3060ìš© K-Fold êµì°¨ ê²€ì¦ ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - Fold ìˆ˜: {n_splits}")
    print(f"   - ì•™ìƒë¸” ë°©ë²•: {ensemble_method}")
    print(f"   - ê³„ì¸µí™”: í™œì„±í™”")
    print(f"   - LoRA: í™œì„±í™”")

    return config


def create_kfold_config_for_high_performance(n_splits=10, ensemble_method='weighted'):
    """ê³ ì„±ëŠ¥ GPUìš© K-Fold êµì°¨ ê²€ì¦ ì„¤ì • ìƒì„±"""
    config_manager = ConfigManager()

    # ê³ ì„±ëŠ¥ ì„¤ì •ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
    config = config_manager.create_optimized_config(
        hardware_key='rtx4090',
        model_key='bart-large'
    )

    # K-Fold ì„¤ì •
    config['kfold'] = {
        'enabled': True,
        'n_splits': n_splits,
        'stratified': True,
        'random_state': 42,
        'ensemble_method': ensemble_method,
        'save_individual_models': True,
        'use_ensemble_inference': True
    }

    # ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì •
    config['training'].update({
        'num_train_epochs': 12,
        'per_device_train_batch_size': 16,
        'per_device_eval_batch_size': 16,
        'gradient_accumulation_steps': 2,
        'learning_rate': 5e-6,  # ë” ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì„± í™•ë³´
        'warmup_ratio': 0.05,
        'early_stopping_patience': 3,
        'evaluation_strategy': 'steps',
        'eval_steps': 200,
        'save_steps': 200,
        'logging_steps': 100
    })

    print("ğŸš€ ê³ ì„±ëŠ¥ GPUìš© K-Fold êµì°¨ ê²€ì¦ ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - Fold ìˆ˜: {n_splits}")
    print(f"   - ì•™ìƒë¸” ë°©ë²•: {ensemble_method}")
    print(f"   - Full Fine-tuning ì‚¬ìš©")

    return config


def create_fast_kfold_config_for_rtx3060(n_splits=3, ensemble_method='best'):
    """RTX 3060ìš© ë¹ ë¥¸ K-Fold ì„¤ì • ìƒì„± (ì‹¤í—˜ìš©)"""
    config_manager = ConfigManager()

    # ë¹ ë¥¸ ì„¤ì •ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
    config = create_balanced_config_for_rtx3060()

    # ë¹ ë¥¸ K-Fold ì„¤ì •
    config['kfold'] = {
        'enabled': True,
        'n_splits': n_splits,  # ì ì€ fold ìˆ˜
        'stratified': False,  # ê³„ì¸µí™” ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        'random_state': 42,
        'ensemble_method': ensemble_method,  # ìµœê³  ëª¨ë¸ë§Œ ì‚¬ìš©
        'save_individual_models': False,  # ì €ì¥ ê³µê°„ ì ˆì•½
        'use_ensemble_inference': False  # ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡ 
    }

    # ë©”ëª¨ë¦¬ ì•ˆì „ í•™ìŠµ ì„¤ì • (RTX 3060 ë…¸íŠ¸ë¶ ìµœì í™”)
    config['training'].update({
        'num_train_epochs': 4,  # ë” ì ì€ ì—í¬í¬
        'per_device_train_batch_size': 6,  # ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ
        'per_device_eval_batch_size': 6,
        'gradient_accumulation_steps': 4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ ë³´ìƒ
        'early_stopping_patience': 1,  # ë§¤ìš° ë¹ ë¥¸ ì¡°ê¸° ì¢…ë£Œ
        'evaluation_strategy': 'epoch',
        'save_strategy': 'epoch',
        'logging_steps': 20,
        'dataloader_num_workers': 0,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        'dataloader_pin_memory': False,  # ë©”ëª¨ë¦¬ í•€ ë¹„í™œì„±í™”
        'remove_unused_columns': True,  # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        'fp16': True,  # FP16 ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        'gradient_checkpointing': True,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        'max_grad_norm': 1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    })

    # í† í¬ë‚˜ì´ì € ì„¤ì •ë„ ë©”ëª¨ë¦¬ ì ˆì•½
    config['tokenizer'].update({
        'encoder_max_len': 256,  # ì‹œí€€ìŠ¤ ê¸¸ì´ ë‹¨ì¶•
        'decoder_max_len': 64,   # ë””ì½”ë” ê¸¸ì´ ë‹¨ì¶•
    })

    print("âš¡ RTX 3060ìš© ë¹ ë¥¸ K-Fold ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - Fold ìˆ˜: {n_splits} (ë¹ ë¥¸ ì‹¤í—˜ìš©)")
    print(f"   - ì•™ìƒë¸” ë°©ë²•: {ensemble_method}")
    print(f"   - ìµœì í™”: ì†ë„ ìš°ì„ ")

    return config
