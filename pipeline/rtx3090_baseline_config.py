"""
RTX 3090ìš© Baseline ì„¤ì • (baseline.ipynb ê¸°ë°˜)
ê³ ì„±ëŠ¥ GPUì—ì„œ baseline.ipynbì™€ ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
"""

from .config_manager import ConfigManager
from transformers import AutoTokenizer


def create_rtx3090_baseline_config():
    """RTX 3090ìš© Baseline ì„¤ì • ìƒì„± (baseline.ipynb ê¸°ë°˜)"""
    config_manager = ConfigManager()

    # baseline.ipynbì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
    model_name = "digit82/kobart-summarization"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ë°ì´í„° ê²½ë¡œ ìë™ ê°ì§€
    data_path = config_manager.find_latest_augmented_data_path()

    config = {
        "general": {
            "data_path": data_path,
            "model_name": model_name,
            "output_dir": "./model_output/",
            "hardware": "rtx3090"
        },
        "tokenizer": {
            "encoder_max_len": 512,  # baseline.ipynbì™€ ë™ì¼
            "decoder_max_len": 100,  # baseline.ipynbì™€ ë™ì¼
            "bos_token": f"{tokenizer.bos_token}",
            "eos_token": f"{tokenizer.eos_token}",
            # baseline.ipynbì˜ special_tokens ì‚¬ìš©
            "special_tokens": [
                '#Person1#', '#Person2#', '#Person3#',
                '#PhoneNumber#', '#Address#', '#DateOfBirth#',
                '#PassportNumber#', '#SSN#', '#CardNumber#',
                '#CarNumber#', '#Email#'
            ],
            "padding": "max_length",
            "truncation": True,
            "return_attention_mask": True,
            "return_token_type_ids": False
        },
        "training": {
            # baseline.ipynbì˜ í•™ìŠµ ì„¤ì • ê·¸ëŒ€ë¡œ ì ìš©
            "overwrite_output_dir": True,
            "num_train_epochs": 40,  # baseline.ipynbì™€ ë™ì¼
            "learning_rate": 1e-5,   # baseline.ipynbì™€ ë™ì¼
            "per_device_train_batch_size": 50,  # baseline.ipynbì™€ ë™ì¼ (RTX 3090ì´ë¯€ë¡œ ê°€ëŠ¥)
            "per_device_eval_batch_size": 32,   # baseline.ipynbì™€ ë™ì¼
            "warmup_ratio": 0.1,     # baseline.ipynbì™€ ë™ì¼
            "weight_decay": 0.01,    # baseline.ipynbì™€ ë™ì¼
            "lr_scheduler_type": 'cosine',  # baseline.ipynbì™€ ë™ì¼
            "optim": 'adamw_torch',  # baseline.ipynbì™€ ë™ì¼
            "gradient_accumulation_steps": 1,  # baseline.ipynbì™€ ë™ì¼

            # í‰ê°€ ë° ì €ì¥ ì „ëµ (K-Fold ìµœì í™”)
            "evaluation_strategy": 'steps',  # K-Foldì—ì„œ ë” ìì£¼ í‰ê°€
            "eval_strategy": 'steps',
            "eval_steps": 500,               # 200 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
            "save_strategy": 'steps',        # K-Foldì—ì„œ ë” ìì£¼ ì €ì¥
            "save_steps": 500,               # 200 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
            "save_total_limit": 3,           # K-Foldì—ì„œ ì €ì¥ ê³µê°„ ì ˆì•½

            # ì„±ëŠ¥ ìµœì í™”
            "fp16": True,                    # baseline.ipynbì™€ ë™ì¼
            "load_best_model_at_end": True,  # baseline.ipynbì™€ ë™ì¼
            "metric_for_best_model": "eval_final_score",
            "greater_is_better": True,

            # ê¸°íƒ€ ì„¤ì •
            "seed": 42,                      # baseline.ipynbì™€ ë™ì¼
            "logging_dir": "./logs",         # baseline.ipynbì™€ ë™ì¼
            "logging_strategy": "steps",     # K-Foldì—ì„œ ë” ìì£¼ ë¡œê¹…
            "logging_steps": 50,             # ë” ìì£¼ ë¡œê¹…

            # ìƒì„± ê´€ë ¨ ì„¤ì •
            "predict_with_generate": True,   # baseline.ipynbì™€ ë™ì¼
            "generation_max_length": 100,    # baseline.ipynbì™€ ë™ì¼

            # í•™ìŠµ/í‰ê°€ í™œì„±í™”
            "do_train": True,                # baseline.ipynbì™€ ë™ì¼
            "do_eval": True,                 # baseline.ipynbì™€ ë™ì¼

            # Early Stopping (K-Fold ìµœì í™”)
            "early_stopping_patience": 5,   # K-Foldì—ì„œ ë” ë¹ ë¥¸ ì¡°ê¸° ì¢…ë£Œ
            "early_stopping_threshold": 0.001,

            # ë¡œê¹… ì„¤ì •
            "report_to": "none",             # baseline.ipynbì™€ ë™ì¼

            # RTX 3090 ìµœì í™” ì¶”ê°€ ì„¤ì •
            "dataloader_num_workers": 4,
            "dataloader_pin_memory": True,
            "dataloader_drop_last": False,
            "ignore_data_skip": True,

            # ë©”ëª¨ë¦¬ ìµœì í™” (RTX 3090ì€ 24GB VRAM)
            "max_grad_norm": 1.0,
            "gradient_checkpointing": False,  # RTX 3090ì€ ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë¯€ë¡œ ë¹„í™œì„±í™”

            "output_dir": "./model_output/"
        },
        "wandb": {
            "entity": "your_entity",
            "project": "dialogue_summarization_baseline",
            "name": "rtx3090_baseline_run"
        },
        "inference": {
            "ckt_path": "./model_output/",
            "result_path": "./prediction/",
            "no_repeat_ngram_size": 2,       # baseline.ipynbì™€ ë™ì¼
            "early_stopping": True,          # baseline.ipynbì™€ ë™ì¼
            "generate_max_length": 100,      # baseline.ipynbì™€ ë™ì¼
            "num_beams": 4,                  # baseline.ipynbì™€ ë™ì¼
            "batch_size": 32,                # baseline.ipynbì™€ ë™ì¼
            # baseline.ipynbì˜ remove_tokens ì‚¬ìš©
            "remove_tokens": ['<usr>', f"{tokenizer.bos_token}", f"{tokenizer.eos_token}", f"{tokenizer.pad_token}"]
        },
        # LoRA ë¹„í™œì„±í™” (baseline.ipynbëŠ” Full Fine-tuning)
        "lora": {
            "enabled": False,
            "use_qlora": False
        },
        # K-Fold í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ê¸°ë³¸ í™œì„±í™”)
        "kfold": {
            "enabled": False,
            "n_splits": 5,
            "stratified": True,  # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê³„ì¸µí™”
            "random_state": 42,
            "ensemble_method": "voting",
            "save_individual_models": True,
            "use_ensemble_inference": True
        }
    }

    print("ğŸš€ RTX 3090 Baseline + K-Fold ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ğŸ“‹ ì£¼ìš” ì„¤ì •:")
    print(f"   - ëª¨ë¸: {model_name}")
    print(f"   - ì—í¬í¬: {config['training']['num_train_epochs']}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config['training']['per_device_train_batch_size']}")
    print(f"   - í•™ìŠµë¥ : {config['training']['learning_rate']}")
    print(f"   - ì¸ì½”ë” ê¸¸ì´: {config['tokenizer']['encoder_max_len']}")
    print(f"   - ë””ì½”ë” ê¸¸ì´: {config['tokenizer']['decoder_max_len']}")
    print(f"   - LoRA: ë¹„í™œì„±í™” (Full Fine-tuning)")
    print(f"   - K-Fold: í™œì„±í™” ({config['kfold']['n_splits']} folds, {config['kfold']['ensemble_method']} ì•™ìƒë¸”)")
    print(f"   - ê³„ì¸µí™”: {config['kfold']['stratified']}")

    return config


def create_rtx3090_baseline_kfold_config(n_splits=5, ensemble_method='voting'):
    """RTX 3090ìš© Baseline + K-Fold ì„¤ì •"""
    config = create_rtx3090_baseline_config()

    # K-Fold í™œì„±í™”
    config['kfold'] = {
        'enabled': True,
        'n_splits': n_splits,
        'stratified': True,
        'random_state': 42,
        'ensemble_method': ensemble_method,
        'save_individual_models': True,
        'use_ensemble_inference': True
    }

    # K-Foldì— ë§ê²Œ í•™ìŠµ ì„¤ì • ì¡°ì •
    config['training'].update({
        'num_train_epochs': 20,  # foldë³„ë¡œ ì¡°ê¸ˆ ì¤„ì„
        'early_stopping_patience': 2,
        'save_total_limit': 3,
        'evaluation_strategy': 'steps',
        'eval_strategy': 'steps',
        'eval_steps': 200,
        'save_steps': 200,
        'logging_steps': 100
    })

    print(f"ğŸ”„ RTX 3090 Baseline + K-Fold ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   - Fold ìˆ˜: {n_splits}")
    print(f"   - ì•™ìƒë¸” ë°©ë²•: {ensemble_method}")
    print(f"   - ì—í¬í¬ ìˆ˜: {config['training']['num_train_epochs']} (K-Foldìš© ì¡°ì •)")

    return config


def create_rtx3090_baseline_fast_config():
    """RTX 3090ìš© ë¹ ë¥¸ Baseline ì„¤ì • (ì‹¤í—˜ìš©)"""
    config = create_rtx3090_baseline_config()

    # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ì„¤ì • ì¡°ì •
    config['training'].update({
        'num_train_epochs': 10,  # ì—í¬í¬ ì¤„ì„
        'per_device_train_batch_size': 64,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (RTX 3090 í™œìš©)
        'per_device_eval_batch_size': 64,
        'early_stopping_patience': 2,
        'evaluation_strategy': 'steps',
        'eval_strategy': 'steps',
        'eval_steps': 100,
        'save_steps': 100,
        'logging_steps': 50
    })

    print("âš¡ RTX 3090 ë¹ ë¥¸ Baseline ì„¤ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   - ìµœì í™”: ë¹ ë¥¸ ì‹¤í—˜ìš©")
    print(f"   - ì—í¬í¬: {config['training']['num_train_epochs']}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config['training']['per_device_train_batch_size']}")

    return config
