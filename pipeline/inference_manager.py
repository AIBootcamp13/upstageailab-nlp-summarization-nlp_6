"""
ì¶”ë¡  ê´€ë¦¬ ëª¨ë“ˆ
"""

import torch
import pandas as pd
import os
from torch.utils.data import DataLoader
from tqdm import tqdm

class InferenceManager:
    """ì¶”ë¡  ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def run_inference(self, model, tokenizer, data_processor):
        """ì¶”ë¡  ì‹¤í–‰ (ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©)"""
        print("=" * 80)
        print("ëª¨ë¸ ì¶”ë¡  ì‹œì‘ (ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©)")
        print("=" * 80)
        
        model.eval()
        
        # ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        test_data, test_dataset = data_processor.prepare_test_dataset(tokenizer, use_original=True)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        dataloader = DataLoader(
            test_dataset, 
            batch_size=self.config['inference']['batch_size'],
            shuffle=False
        )
        
        summaries = []
        text_ids = []
        
        print(f"ì´ {len(dataloader)} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="ì¶”ë¡  ì§„í–‰"):
                # ID ì €ì¥
                text_ids.extend(batch['ID'])
                
                # ìƒì„±
                generated_ids = model.generate(
                    input_ids=batch['input_ids'].to(self.device),
                    attention_mask=batch['attention_mask'].to(self.device),
                    no_repeat_ngram_size=self.config['inference']['no_repeat_ngram_size'],
                    early_stopping=self.config['inference']['early_stopping'],
                    max_length=self.config['inference']['generate_max_length'],
                    num_beams=self.config['inference']['num_beams'],
                )
                
                # ë””ì½”ë”©
                for ids in generated_ids:
                    result = tokenizer.decode(ids, skip_special_tokens=False)
                    summaries.append(result)
        
        # í›„ì²˜ë¦¬
        cleaned_summaries = self._clean_summaries(summaries)
        
        # ê²°ê³¼ ì €ì¥ (ì´ì œ ëˆ„ë½ ìƒ˜í”Œ ì²˜ë¦¬ê°€ ë¶ˆí•„ìš”í•¨)
        output_df = self._save_results_simple(test_data, cleaned_summaries)
        
        print("=" * 80)
        print("ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ!")
        print(f"ê²°ê³¼ê°€ {self.config['inference']['result_path']}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 80)
        
        return output_df
    
    def _clean_summaries(self, summaries):
        """ìš”ì•½ë¬¸ í›„ì²˜ë¦¬"""
        remove_tokens = self.config['inference']['remove_tokens']
        cleaned_summaries = summaries.copy()
        
        for token in remove_tokens:
            cleaned_summaries = [
                sentence.replace(token, " ").strip() 
                for sentence in cleaned_summaries
            ]
        
        return cleaned_summaries
    
    def _save_results(self, test_data, summaries):
        """ê²°ê³¼ ì €ì¥ (ëˆ„ë½ëœ ìƒ˜í”Œ ìë™ ì²˜ë¦¬ í¬í•¨)"""
        output_df = pd.DataFrame({
            "fname": test_data['fname'],
            "summary": summaries,
        })
        
        # sample_submissionê³¼ ë¹„êµí•˜ì—¬ ëˆ„ë½ëœ ìƒ˜í”Œ ì²˜ë¦¬
        sample_submission_path = "data/sample_submission.csv"
        if os.path.exists(sample_submission_path):
            try:
                sample_df = pd.read_csv(sample_submission_path)
                required_fnames = sample_df['fname'].tolist()
                existing_fnames = output_df['fname'].tolist()
                missing_fnames = [fname for fname in required_fnames if fname not in existing_fnames]
                
                if missing_fnames:
                    print(f"âš ï¸ ëˆ„ë½ëœ ìƒ˜í”Œ {len(missing_fnames)}ê°œ ë°œê²¬, ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.")
                    
                    # ëˆ„ë½ëœ ìƒ˜í”Œë“¤ì„ ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ì¶”ê°€
                    missing_data = []
                    for fname in missing_fnames:
                        missing_data.append({
                            'fname': fname,
                            'summary': 'ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤.'
                        })
                    
                    missing_df = pd.DataFrame(missing_data)
                    output_df = pd.concat([output_df, missing_df], ignore_index=True)
                    
                    # sample_submission ìˆœì„œì— ë§ì¶° ì •ë ¬
                    output_df = output_df.set_index('fname').loc[required_fnames].reset_index()
                    
                    print(f"âœ… ì´ {len(output_df)}ê°œ ìƒ˜í”Œë¡œ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"âš ï¸ sample_submission ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        result_path = self.config['inference']['result_path']
        os.makedirs(result_path, exist_ok=True)
        
        # CSV ì €ì¥
        output_file = os.path.join(result_path, "output.csv")
        output_df.to_csv(output_file, index=False)
        
        print(f"ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_file}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\n=== ì¶”ë¡  ê²°ê³¼ ìƒ˜í”Œ ===")
        for i in range(min(3, len(output_df))):
            print(f"\n[ìƒ˜í”Œ {i+1}]")
            print(f"íŒŒì¼ëª…: {output_df.iloc[i]['fname']}")
            print(f"ìš”ì•½: {output_df.iloc[i]['summary']}")
        
        return output_df
    
    def _save_results_simple(self, test_data, summaries):
        """ê°„ë‹¨í•œ ê²°ê³¼ ì €ì¥ (ì›ë³¸ ë°ì´í„° ì‚¬ìš©ì‹œ ëˆ„ë½ ìƒ˜í”Œ ì—†ìŒ)"""
        output_df = pd.DataFrame({
            "fname": test_data['fname'],
            "summary": summaries,
        })
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        result_path = self.config['inference']['result_path']
        os.makedirs(result_path, exist_ok=True)
        
        # CSV ì €ì¥
        output_file = os.path.join(result_path, "output.csv")
        output_df.to_csv(output_file, index=False)
        
        print(f"âœ… ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(output_df)}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\n=== ì¶”ë¡  ê²°ê³¼ ìƒ˜í”Œ ===")
        for i in range(min(3, len(output_df))):
            print(f"\n[ìƒ˜í”Œ {i+1}]")
            print(f"íŒŒì¼ëª…: {output_df.iloc[i]['fname']}")
            print(f"ìš”ì•½: {output_df.iloc[i]['summary']}")
        
        return output_df
    
    def evaluate_with_reference(self, predictions, references):
        """ì°¸ì¡° ìš”ì•½ê³¼ ë¹„êµ í‰ê°€"""
        from rouge import Rouge
        
        rouge = Rouge()
        
        try:
            scores = rouge.get_scores(predictions, references, avg=True)
            
            print("\n=== í‰ê°€ ê²°ê³¼ ===")
            for metric, values in scores.items():
                print(f"{metric.upper()}:")
                print(f"  Precision: {values['p']:.4f}")
                print(f"  Recall: {values['r']:.4f}")
                print(f"  F1-Score: {values['f']:.4f}")
            
            return scores
            
        except Exception as e:
            print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def generate_single_summary(self, model, tokenizer, dialogue):
        """ë‹¨ì¼ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ ìƒì„±"""
        model.eval()
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            dialogue,
            return_tensors='pt',
            max_length=self.config['tokenizer']['encoder_max_len'],
            truncation=True,
            padding=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                no_repeat_ngram_size=self.config['inference']['no_repeat_ngram_size'],
                early_stopping=self.config['inference']['early_stopping'],
                max_length=self.config['inference']['generate_max_length'],
                num_beams=self.config['inference']['num_beams'],
            )
        
        # ë””ì½”ë”©
        summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        # í›„ì²˜ë¦¬
        for token in self.config['inference']['remove_tokens']:
            summary = summary.replace(token, " ")
        
        return summary.strip()

class InteractiveInference:
    """ëŒ€í™”í˜• ì¶”ë¡  í´ë˜ìŠ¤"""
    def __init__(self, config, model, tokenizer):
        self.config = config
        self.model = model
        self.tokenizer = tokenizer
        self.inference_manager = InferenceManager(config)
    
    def start_interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘"""
        print("=" * 80)
        print("ëŒ€í™”í˜• ìš”ì•½ ìƒì„± ëª¨ë“œ")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 80)
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                dialogue = input("\nëŒ€í™”ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\n> ")
                
                if dialogue.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not dialogue.strip():
                    print("ëŒ€í™”ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ìš”ì•½ ìƒì„±
                print("\nìš”ì•½ ìƒì„± ì¤‘...")
                summary = self.inference_manager.generate_single_summary(
                    self.model, self.tokenizer, dialogue
                )
                
                print(f"\nìƒì„±ëœ ìš”ì•½:\n{summary}")
                print("-" * 50)
                
            except KeyboardInterrupt:
                print("\n\nëŒ€í™”í˜• ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from config_manager import ConfigManager
    
    config_manager = ConfigManager()
    config = config_manager.load_config()
    
    inference_manager = InferenceManager(config)
    print("ì¶”ë¡  ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ")