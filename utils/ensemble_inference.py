"""
ì•™ìƒë¸” ê¸°ë²•ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ë” ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import numpy as np
from collections import Counter

class EnsembleInference:
    def __init__(self, model_paths, weights=None):
        """
        ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_paths: ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            weights: ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ë™ì¼ ê°€ì¤‘ì¹˜)
        """
        self.model_paths = model_paths
        self.weights = weights or [1.0] * len(model_paths)
        self.models = []
        self.tokenizers = []
        
        # ëª¨ë¸ë“¤ ë¡œë“œ
        self.load_models()
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        
        print("ğŸ“¦ ì•™ìƒë¸” ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        for i, model_path in enumerate(self.model_paths):
            try:
                print(f"  {i+1}. {model_path} ë¡œë“œ ì¤‘...")
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
                model.to(device)
                model.eval()
                
                self.tokenizers.append(tokenizer)
                self.models.append(model)
                print("     âœ… ì„±ê³µ")
                
            except Exception as e:
                print(f"     âŒ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ëª¨ë¸ì€ ì œì™¸
                self.weights.pop(i)
        
        print(f"âœ… ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.weights)
        self.weights = [w / total_weight for w in self.weights]
        print(f"ğŸ“Š ëª¨ë¸ ê°€ì¤‘ì¹˜: {self.weights}")
    
    def generate_single_prediction(self, model, tokenizer, input_text, generation_params):
        """ë‹¨ì¼ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±"""
        
        device = next(model.parameters()).device
        
        # í† í°í™”
        inputs = tokenizer(
            input_text,
            max_length=512,
            truncation=True,
            return_tensors='pt'
        ).to(device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(**inputs, **generation_params)
        
        # ë””ì½”ë”©
        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í›„ì²˜ë¦¬
        summary = summary.strip()
        if summary.startswith('ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:'):
            summary = summary.replace('ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:', '').strip()
        
        return summary
    
    def voting_ensemble(self, predictions):
        """íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”"""
        
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        all_words = []
        for pred in predictions:
            words = pred.split()
            all_words.extend(words)
        
        # ë¹ˆë„ ê¸°ë°˜ íˆ¬í‘œ
        word_counts = Counter(all_words)
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë“¤ë¡œ ìš”ì•½ ì¬êµ¬ì„±
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ê° ì˜ˆì¸¡ì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë“¤ ìš°ì„ 
        common_words = []
        for word, count in word_counts.most_common():
            if count >= len(predictions) // 2:  # ì ˆë°˜ ì´ìƒì˜ ëª¨ë¸ì—ì„œ ì‚¬ìš©
                common_words.append(word)
        
        # ì›ë³¸ ì˜ˆì¸¡ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì¬êµ¬ì„±
        longest_pred = max(predictions, key=len)
        
        # ê³µí†µ ë‹¨ì–´ë“¤ì´ í¬í•¨ëœ ì˜ˆì¸¡ ì„ íƒ
        best_pred = longest_pred
        max_common_count = 0
        
        for pred in predictions:
            common_count = sum(1 for word in pred.split() if word in common_words)
            if common_count > max_common_count:
                max_common_count = common_count
                best_pred = pred
        
        return best_pred
    
    def length_weighted_ensemble(self, predictions):
        """ê¸¸ì´ ê°€ì¤‘ ì•™ìƒë¸”"""
        
        # ì ì ˆí•œ ê¸¸ì´ì˜ ì˜ˆì¸¡ë“¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        target_length = 50  # ëª©í‘œ ê¸¸ì´
        
        weighted_scores = []
        for pred in predictions:
            length_score = 1.0 / (1.0 + abs(len(pred.split()) - target_length) / target_length)
            weighted_scores.append(length_score)
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ˆì¸¡ ì„ íƒ
        best_idx = np.argmax(weighted_scores)
        return predictions[best_idx]
    
    def semantic_ensemble(self, predictions):
        """ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ì•™ìƒë¸”"""
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ì„±
        # ì‹¤ì œë¡œëŠ” sentence embeddingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
        
        keyword_scores = []
        important_keywords = ['ì—ê²Œ', 'ëŒ€í•´', 'í•©ë‹ˆë‹¤', 'ì„¤ëª…', 'ìš”ì²­', 'ì´ì•¼ê¸°', 'ì œì•ˆ']
        
        for pred in predictions:
            score = sum(1 for keyword in important_keywords if keyword in pred)
            keyword_scores.append(score)
        
        # í‚¤ì›Œë“œ ì ìˆ˜ê°€ ë†’ì€ ì˜ˆì¸¡ ì„ íƒ
        if max(keyword_scores) > 0:
            best_idx = np.argmax(keyword_scores)
            return predictions[best_idx]
        else:
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸¸ì´ ê¸°ë°˜ ì„ íƒ
            return self.length_weighted_ensemble(predictions)
    
    def ensemble_predict(self, input_text, method='voting'):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        
        if not self.models:
            raise ValueError("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìƒì„± íŒŒë¼ë¯¸í„°
        generation_params = {
            'max_length': 128,
            'min_length': 15,
            'num_beams': 5,
            'length_penalty': 1.2,
            'repetition_penalty': 1.1,
            'early_stopping': True,
            'do_sample': False,
            'no_repeat_ngram_size': 2,
        }
        
        # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±
        predictions = []
        for i, (model, tokenizer) in enumerate(zip(self.models, self.tokenizers)):
            try:
                pred = self.generate_single_prediction(
                    model, tokenizer, input_text, generation_params
                )
                predictions.append(pred)
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ {i+1} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        
        if not predictions:
            return "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì•™ìƒë¸” ë°©ë²• ì ìš©
        if method == 'voting':
            return self.voting_ensemble(predictions)
        elif method == 'length_weighted':
            return self.length_weighted_ensemble(predictions)
        elif method == 'semantic':
            return self.semantic_ensemble(predictions)
        else:
            # ê¸°ë³¸: ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë°˜í™˜
            return predictions[0]

def create_ensemble_models():
    """ì•™ìƒë¸”ìš© ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ ì œì•ˆ"""
    
    print("ğŸ¯ ì•™ìƒë¸”ìš© ëª¨ë¸ ìƒì„± ì „ëµ")
    print("="*50)
    
    strategies = [
        {
            "ëª¨ë¸": "Model 1 - KoBART Large",
            "ì„¤ì •": "gogamza/kobart-large-v2 + ë†’ì€ í•™ìŠµë¥ ",
            "íŠ¹ì§•": "í•œêµ­ì–´ íŠ¹í™”, ê³µê²©ì  í•™ìŠµ"
        },
        {
            "ëª¨ë¸": "Model 2 - BART Base",
            "ì„¤ì •": "facebook/bart-base + ë³´ìˆ˜ì  í•™ìŠµë¥ ",
            "íŠ¹ì§•": "ì•ˆì •ì  ì„±ëŠ¥, ê¸´ í•™ìŠµ"
        },
        {
            "ëª¨ë¸": "Model 3 - T5 Base", 
            "ì„¤ì •": "t5-base + ë‹¤ë¥¸ ì „ì²˜ë¦¬",
            "íŠ¹ì§•": "ë‹¤ë¥¸ ì•„í‚¤í…ì²˜, ë‹¤ì–‘ì„± í™•ë³´"
        }
    ]
    
    for strategy in strategies:
        print(f"\nğŸ“¦ {strategy['ëª¨ë¸']}")
        print(f"  ì„¤ì •: {strategy['ì„¤ì •']}")
        print(f"  íŠ¹ì§•: {strategy['íŠ¹ì§•']}")
    
    print("\nğŸ’¡ ì•™ìƒë¸” íš¨ê³¼:")
    print("  - ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ 3-8ì  í–¥ìƒ ì˜ˆìƒ")
    print("  - ì•ˆì •ì„± ì¦ê°€")
    print("  - ë‹¤ì–‘í•œ ê´€ì ì˜ ìš”ì•½ ìƒì„±")

def discover_trained_models():
    """model_output í´ë”ì—ì„œ í•™ìŠµëœ ëª¨ë¸ë“¤ ìë™ íƒì§€"""
    
    print("ğŸ” í•™ìŠµëœ ëª¨ë¸ íƒì§€ ì¤‘...")
    
    if not os.path.exists('model_output'):
        print("âŒ model_output í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    model_paths = []
    model_dirs = [d for d in os.listdir('model_output') if os.path.isdir(f'model_output/{d}')]
    
    for model_dir in sorted(model_dirs):
        model_path = f'model_output/{model_dir}/final'
        info_path = f'model_output/{model_dir}/model_info.json'
        
        if os.path.exists(model_path) and os.path.exists(info_path):
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ
            try:
                import json
                with open(info_path, 'r', encoding='utf-8') as f:
                    model_info = json.load(f)
                
                model_paths.append({
                    'path': model_path,
                    'info': model_info,
                    'timestamp': model_info.get('timestamp', ''),
                    'model_name': model_info.get('model_short', 'unknown')
                })
                
                print(f"âœ… ë°œê²¬ëœ ëª¨ë¸: {model_dir}")
                print(f"   - ëª¨ë¸: {model_info.get('model_short', 'unknown')}")
                print(f"   - ì‹œê°„: {model_info.get('timestamp', 'unknown')}")
                
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {model_dir} - {e}")
        else:
            print(f"âŒ ë¶ˆì™„ì „í•œ ëª¨ë¸: {model_dir}")
    
    return model_paths

def run_ensemble_inference():
    """ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰"""
    
    print("ğŸ¯ ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰")
    print("="*50)
    
    # í•™ìŠµëœ ëª¨ë¸ë“¤ ìë™ íƒì§€
    discovered_models = discover_trained_models()
    
    if len(discovered_models) < 2:
        print("âš ï¸ ì•™ìƒë¸”ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"í˜„ì¬ ë°œê²¬ëœ ëª¨ë¸: {len(discovered_models)}ê°œ")
        print("ğŸ’¡ scripts/quick_train_optimized.pyë¥¼ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ëª¨ë¸ ê²½ë¡œë§Œ ì¶”ì¶œ
    existing_models = [model['path'] for model in discovered_models]
    
    print(f"\nğŸ“Š ì•™ìƒë¸”ì— ì‚¬ìš©í•  ëª¨ë¸: {len(existing_models)}ê°œ")
    for i, model in enumerate(discovered_models):
        print(f"  {i+1}. {model['model_name']} ({model['timestamp']})")
    
    # ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì´ˆê¸°í™”
    ensemble = EnsembleInference(existing_models)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_df = pd.read_csv('data/test.csv')
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)} ìƒ˜í”Œ")
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰
    predictions = []
    
    print("ğŸ”„ ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì§„í–‰ ì¤‘...")
    for idx, row in test_df.iterrows():
        # ì…ë ¥ ìµœì í™”
        dialogue = row['dialogue']
        dialogue = dialogue.replace('A:', '[í™”ì1]:').replace('B:', '[í™”ì2]:')
        input_text = f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”: {dialogue}"
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        try:
            pred = ensemble.ensemble_predict(input_text, method='semantic')
            predictions.append(pred)
        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ {idx} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            predictions.append("ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if (idx + 1) % 50 == 0:
            print(f"  ì§„í–‰ë¥ : {idx + 1}/{len(test_df)} ({(idx + 1)/len(test_df)*100:.1f}%)")
    
    # ê²°ê³¼ ì €ì¥
    print("ğŸ’¾ ì•™ìƒë¸” ê²°ê³¼ ì €ì¥ ì¤‘...")
    submission = pd.read_csv('data/sample_submission.csv')
    submission['summary'] = predictions
    submission.to_csv('ensemble_submission.csv', index=False)
    
    print("âœ… ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì™„ë£Œ!")
    print("ğŸ“ ê²°ê³¼ íŒŒì¼: ensemble_submission.csv")
    
    # ìƒ˜í”Œ ê²°ê³¼ í™•ì¸
    print("\nğŸ“‹ ì•™ìƒë¸” ê²°ê³¼ ìƒ˜í”Œ:")
    for i in range(min(3, len(predictions))):
        print(f"  {i+1}. {predictions[i]}")

if __name__ == "__main__":
    import os
    
    print("ğŸ¯ ì•™ìƒë¸” ê¸°ë²•ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ")
    print("ì˜ˆìƒ íš¨ê³¼: +3-8ì ")
    print("="*60)
    
    # ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì „ëµ ì•ˆë‚´
    create_ensemble_models()
    
    print("\n" + "="*60)
    
    # ì•™ìƒë¸” ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰
    run_ensemble_inference()