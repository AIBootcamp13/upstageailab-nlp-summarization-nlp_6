#!/usr/bin/env python3
"""
í†µí•©ëœ Final Score ê¸°ë°˜ ë©”íŠ¸ë¦­ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
ëª¨ë“  í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì¼ê´€ëœ ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ìœ„í•œ ëª¨ë“ˆ
"""

import numpy as np
from rouge import Rouge
import logging

logger = logging.getLogger(__name__)

def compute_unified_metrics(predictions, labels, tokenizer, config=None, verbose=True):
    """
    í†µí•©ëœ Final Score ê¸°ë°˜ ë©”íŠ¸ë¦­ ê³„ì‚°

    Args:
        predictions: ì˜ˆì¸¡ í† í° ID ë°°ì—´
        labels: ì •ë‹µ í† í° ID ë°°ì—´
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (remove_tokens í¬í•¨)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        Dict[str, float]: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ë“¤
    """
    try:
        # predict_with_generate=Trueë¥¼ ì‚¬ìš©í•˜ë©´ predictionsëŠ” ìƒì„±ëœ í† í° IDì…ë‹ˆë‹¤.
        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # Rouge-Lì˜ F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        rouge = Rouge()

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì—ì„œ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤.
        cleaned_preds = [
            pred.split("### Response:")[1].strip() if "### Response:" in pred else pred.strip()
            for pred in decoded_preds
        ]
        cleaned_labels = [
            label.split("### Response:")[1].strip() if "### Response:" in label else label.strip()
            for label in decoded_labels
        ]

        # ì„¤ì •ì—ì„œ ì œê±°í•  í† í°ë“¤ ê°€ì ¸ì˜¤ê¸°
        if config and 'inference' in config and 'remove_tokens' in config['inference']:
            remove_tokens = config['inference']['remove_tokens']
        else:
            remove_tokens = ['<usr>', '</s>', '<pad>']  # ê¸°ë³¸ê°’

        # ë¶ˆí•„ìš”í•œ í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
        for token in remove_tokens:
            cleaned_preds = [sentence.replace(token, " ") for sentence in cleaned_preds]
            cleaned_labels = [sentence.replace(token, " ") for sentence in cleaned_labels]

        # ë¹„ì–´ìˆëŠ” ì˜ˆì¸¡ì´ë‚˜ ë ˆì´ë¸”ì´ ìˆì„ ê²½ìš° ì ìˆ˜ ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        valid_pairs = [(pred, label) for pred, label in zip(cleaned_preds, cleaned_labels)
                       if pred and label]

        if not valid_pairs:
            if verbose:
                logger.warning("ìœ íš¨í•œ ì˜ˆì¸¡-ë¼ë²¨ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"rouge-1": 0, "rouge-2": 0, "rouge-l": 0, "final_score": 0, "eval_final_score": 0}

        valid_preds, valid_labels = zip(*valid_pairs)

        # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        if verbose:
            print('-'*150)
            if len(valid_preds) > 0:
                print(f"PRED: {valid_preds[0]}")
                print(f"GOLD: {valid_labels[0]}")
            print('-'*150)
            if len(valid_preds) > 1:
                print(f"PRED: {valid_preds[1]}")
                print(f"GOLD: {valid_labels[1]}")
            print('-'*150)
            if len(valid_preds) > 2:
                print(f"PRED: {valid_preds[2]}")
                print(f"GOLD: {valid_labels[2]}")

        scores = rouge.get_scores(list(valid_preds), list(valid_labels), avg=True)

        result = {
            "rouge-1": scores["rouge-1"]["f"],
            "rouge-2": scores["rouge-2"]["f"],
            "rouge-l": scores["rouge-l"]["f"],
        }

        # final_score ê³„ì‚°
        result["final_score"] = result["rouge-1"] + result["rouge-2"] + result["rouge-l"]
        result["eval_final_score"] = result["final_score"]  # early stoppingìš©

        if verbose:
            logger.info(f"Final Score: {result['final_score']:.4f}")

        return result

    except Exception as e:
        logger.error(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {"rouge-1": 0, "rouge-2": 0, "rouge-l": 0, "final_score": 0, "eval_final_score": 0}

def create_unified_compute_metrics_function(tokenizer, config=None):
    """
    í†µí•©ëœ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ìƒì„±ê¸°
    Hugging Face Trainerì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë°˜í™˜

    Args:
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        function: compute_metrics í•¨ìˆ˜
    """
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        return compute_unified_metrics(predictions, labels, tokenizer, config)

    return compute_metrics

def create_legacy_compute_metrics_function(tokenizer, config):
    """
    ê¸°ì¡´ ìŠ¤íƒ€ì¼ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ ìƒì„±ê¸°

    Args:
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        function: compute_metrics í•¨ìˆ˜ (ê¸°ì¡´ ìŠ¤íƒ€ì¼)
    """
    def compute_metrics(config_param, tokenizer_param, pred):
        # ê¸°ì¡´ ìŠ¤íƒ€ì¼ì—ì„œëŠ” pred.predictions, pred.labels í˜•íƒœ
        if hasattr(pred, 'predictions') and hasattr(pred, 'label_ids'):
            predictions = pred.predictions
            labels = pred.label_ids
        else:
            predictions, labels = pred

        return compute_unified_metrics(predictions, labels, tokenizer, config)

    return compute_metrics

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_rouge_score(pred_text, gold_text):
    """
    ë¹ ë¥¸ ROUGE ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ì…ë ¥)

    Args:
        pred_text: ì˜ˆì¸¡ í…ìŠ¤íŠ¸
        gold_text: ì •ë‹µ í…ìŠ¤íŠ¸

    Returns:
        Dict[str, float]: ROUGE ì ìˆ˜ë“¤
    """
    try:
        rouge = Rouge()
        scores = rouge.get_scores([pred_text], [gold_text], avg=True)

        result = {
            "rouge-1": scores["rouge-1"]["f"],
            "rouge-2": scores["rouge-2"]["f"],
            "rouge-l": scores["rouge-l"]["f"],
        }
        result["final_score"] = result["rouge-1"] + result["rouge-2"] + result["rouge-l"]

        return result
    except:
        return {"rouge-1": 0, "rouge-2": 0, "rouge-l": 0, "final_score": 0}

def batch_rouge_scores(pred_texts, gold_texts):
    """
    ë°°ì¹˜ ROUGE ì ìˆ˜ ê³„ì‚°

    Args:
        pred_texts: ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        gold_texts: ì •ë‹µ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        Dict[str, float]: í‰ê·  ROUGE ì ìˆ˜ë“¤
    """
    try:
        rouge = Rouge()
        scores = rouge.get_scores(pred_texts, gold_texts, avg=True)

        result = {
            "rouge-1": scores["rouge-1"]["f"],
            "rouge-2": scores["rouge-2"]["f"],
            "rouge-l": scores["rouge-l"]["f"],
        }
        result["final_score"] = result["rouge-1"] + result["rouge-2"] + result["rouge-l"]

        return result
    except:
        return {"rouge-1": 0, "rouge-2": 0, "rouge-l": 0, "final_score": 0}

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚° í…ŒìŠ¤íŠ¸")

    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    pred = "ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”"
    gold = "ì•ˆë…•í•˜ì„¸ìš” ë‚ ì”¨ê°€ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤"

    scores = quick_rouge_score(pred, gold)
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for key, value in scores.items():
        print(f"  {key}: {value:.4f}")
    print(f"  final_score: {scores['final_score']:.4f}")
